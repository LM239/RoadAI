{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm to detect both load and dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader\n",
    "from pydantic import BaseModel\n",
    "import typing\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import geopy.distance\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from schemas import Machine, Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = '04-27-2023' #DD-MM-YYYY\n",
    "machine_type = 'Truck'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class setting parameters determining when we predict load/dump\n",
    "\n",
    "class criteria(BaseModel):\n",
    "    optimal_K: int = 50                     # Nb of clusters for work areas\n",
    "    meters_from_area: int = 30              # Radius from a cluster center\n",
    "    seconds_for_vector: int = 10            # \"Length\" of vector used to determine if vehicle is reversing\n",
    "    \n",
    "    speed_limit: int = 30                   # Cannot be loading or dumping if speed higher than this. Don't like such a high number.\n",
    "    meters_since_last_activity: int = 500   # Meters driven since last load/dump\n",
    "    \n",
    "    minutes_load: int = 3                   # Look at distance driven last x minutes before a possible load\n",
    "    max_sum_last_x_minutes_load: int = 1000 # Max meters driven during the last x minutes\n",
    "    minutes_dump: int = 3                   # Look at distance driven last x minutes before a possible load\n",
    "    max_sum_last_x_minutes_dump: int = 1000 # Max meters driven during the last x minutes\n",
    "    \n",
    "    inner_prod_threshold: float = 0.80      # A threshold to pick up possible reversal\n",
    "\n",
    "criterias = criteria()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n"
     ]
    }
   ],
   "source": [
    "day = '04-27-2023' #DD-MM-YYYY\n",
    "machine_type = 'Truck'\n",
    "#Loading gps data for selected day and day before\n",
    "trip = dataloader.TripsLoader(day)\n",
    "\n",
    "#Use previous day data to create clustering\n",
    "\n",
    "# Convert the date string to a datetime object\n",
    "date_obj = datetime.strptime(day, \"%m-%d-%Y\")\n",
    "# Subtract one day using timedelta\n",
    "new_date = date_obj - timedelta(days=1)\n",
    "# Format the new date back to the desired format\n",
    "day_before = new_date.strftime(\"%m-%d-%Y\")\n",
    "\n",
    "trip_day_before = dataloader.TripsLoader(day_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_load_dump_clusters():\n",
    "    all_load_positions_for_day_before = []\n",
    "    all_dump_positions_for_day_before = []\n",
    "    for machine_number in trip_day_before._machines.keys():\n",
    "        temp_machine = trip_day_before._machines[machine_number]\n",
    "        if temp_machine.machine_type == machine_type:\n",
    "            all_load_positions_for_day_before.append([trip.load_latlon for trip in temp_machine.trips])\n",
    "            all_dump_positions_for_day_before.append([trip.dump_latlon for trip in temp_machine.trips])\n",
    "\n",
    "    all_load_positions_for_day_before =  [item for sublist in all_load_positions_for_day_before for item in sublist]\n",
    "    all_dump_positions_for_day_before = [item for sublist in all_dump_positions_for_day_before for item in sublist]\n",
    "\n",
    "    # Assuming 'coordinates' is list of tuples [(lat1, lon1), (lat2, lon2), ...]\n",
    "    load_coordinates_array = np.array(all_load_positions_for_day_before)\n",
    "    dump_coordinates_array = np.array(all_dump_positions_for_day_before)\n",
    "\n",
    "    # Fit the KMeans model with the optimal K value\n",
    "    load_kmeans = KMeans(n_clusters=criterias.optimal_K, random_state=42, n_init='auto')\n",
    "    dump_kmeans = KMeans(n_clusters=criterias.optimal_K, random_state=42, n_init='auto')\n",
    "    load_kmeans.fit(load_coordinates_array)\n",
    "    dump_kmeans.fit(dump_coordinates_array)\n",
    "\n",
    "    # Get the coordinates of the cluster centers for the optimal K value\n",
    "    load_cluster_centers = load_kmeans.cluster_centers_\n",
    "    dump_cluster_centers = dump_kmeans.cluster_centers_\n",
    "\n",
    "    return load_cluster_centers, dump_cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coordinates of the cluster centers\n",
    "load_cluster_centers, dump_cluster_centers = generate_load_dump_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class points_times(BaseModel):\n",
    "    points: list[tuple[float, float]] = []\n",
    "    times: list[datetime] = []\n",
    "\n",
    "class predicted_load_dump(BaseModel):\n",
    "    load: points_times = points_times()\n",
    "    dump: points_times = points_times()\n",
    "\n",
    "class stats(BaseModel): #Represents actual data\n",
    "    all_positions: list[Position] = []  # Positions recorded during a day\n",
    "    load: points_times = points_times() # Load points and times\n",
    "    dump: points_times = points_times() # Dump points and times\n",
    "    day_speeds: list[float] = []        # Speeds\n",
    "    day_dists: list[float] = []         # Distances between each recording\n",
    "    day_times: list[datetime] = []      # Timestamp for two above lists\n",
    "    inner_prods: list[float] = []       # Inner product of consecutive normalized driving vectors\n",
    "\n",
    "\n",
    "class automated_load_dump_for_machine():\n",
    "\n",
    "    def __init__(self, \n",
    "                 machine_data: Machine): \n",
    "                #  load_cluster_centers: typing.Any,# Both should be list[tuple[float, float]], should implement \n",
    "                #  dump_cluster_centers: typing.Any) -> None:\n",
    "        \n",
    "        self.machine = machine_data\n",
    "        self.predicted = predicted_load_dump()\n",
    "        self.stats = stats()\n",
    "\n",
    "        all_pos = [trips.positions for trips in self.machine.trips]\n",
    "        self.stats.all_positions = [item for sublist in all_pos for item in sublist]\n",
    "        self.stats.load.points = [trips.load_latlon for trips in self.machine.trips]\n",
    "        self.stats.load.times = [trips.positions[0].timestamp for trips in self.machine.trips]\n",
    "        self.stats.dump.points = [trips.dump_latlon for trips in self.machine.trips]\n",
    "        \n",
    "        actual_dump_times = []\n",
    "        for t in self.machine.trips:        #Not pretty, because we don't have dump time in trip info by default\n",
    "            temp_dump_laton = t.dump_latlon # Must match latlons\n",
    "            for position in t.positions:\n",
    "                if temp_dump_laton == (position.lat, position.lon):\n",
    "                    actual_dump_times.append(position.timestamp)\n",
    "                    break\n",
    "        self.stats.dump.times = actual_dump_times\n",
    "\n",
    "        # self.load_cluster_centers = load_cluster_centers\n",
    "        # self.dump_cluster_centers = dump_cluster_centers\n",
    "\n",
    "        #These four lines should be rewritten, to a class or something\n",
    "        self.entering_load_working_area = []\n",
    "        self.exiting_load_working_area = []\n",
    "        self.entering_dump_working_area = []\n",
    "        self.exiting_dump_working_area = []\n",
    "        self.in_dumping_area: list[int | bool] = []\n",
    "        self.in_loading_area: list[int | bool] = []\n",
    "\n",
    "    def predict(self):\n",
    "\n",
    "        #We know first loading because that is when data begins\n",
    "        self.predicted.load.points.append((self.stats.all_positions[0].lat, self.stats.all_positions[0].lon))\n",
    "        self.predicted.load.times.append(self.stats.all_positions[0].timestamp)\n",
    "\n",
    "        #When true, we are predicting load, else dump. Next prediction will be dump, since we have load above\n",
    "        predicting_load = False\n",
    "\n",
    "        #Initialize variables that keep track of whether or not we are in a usual area for loading or dumping\n",
    "        #As given by load and dump clusters and criterias.meters_from_area\n",
    "        in_load_working_area = False #Probably true, since first position is when loading\n",
    "        in_dump_working_area = False #Maybe false, since first position is when loading\n",
    "\n",
    "        \n",
    "        #We determine the true value of the two above variables\n",
    "        # for coord in self.load_cluster_centers:  #But verify with created clusters of load points from day before\n",
    "        #         if geopy.distance.geodesic(coord, (self.stats.all_positions[0].lat, self.stats.all_positions[0].lon)).m < criterias.meters_from_area:\n",
    "        #             in_load_working_area = True\n",
    "        #             self.in_loading_area.append(1)\n",
    "        #         else:\n",
    "        #             self.in_loading_area.append(0)\n",
    "\n",
    "        # for coord in self.dump_cluster_centers: #But verify with created clusters of load points from day before\n",
    "        #         if geopy.distance.geodesic(coord, (self.stats.all_positions[0].lat, self.stats.all_positions[0].lon)).m < criterias.meters_from_area:\n",
    "        #             in_dump_working_area = True\n",
    "        #             self.in_dumping_area.append(1)\n",
    "        #         else:\n",
    "        #             self.in_dumping_area.append(0)\n",
    "\n",
    "        \n",
    "\n",
    "        #We keep track of how many meters we have driven since last dump or load    \n",
    "        meters_since_last_activity = 0\n",
    "        \n",
    "        #We start predicting. Are going to iterate over all positions, from first to last\n",
    "        for i in range(1,len(self.stats.all_positions[1:])):\n",
    "            \n",
    "            current_pos = self.stats.all_positions[i]\n",
    "            prev_pos = self.stats.all_positions[i-1]\n",
    "\n",
    "            #Seconds passed since last timestamp\n",
    "            seconds_gone = (current_pos.timestamp.to_pydatetime()-prev_pos.timestamp.to_pydatetime()).total_seconds()\n",
    "\n",
    "            if seconds_gone > 0:\n",
    "\n",
    "                #Meters driven since last timestamp\n",
    "                meters_driven = geopy.distance.geodesic((current_pos.lat, current_pos.lon), (prev_pos.lat, prev_pos.lon)).m\n",
    "                meters_since_last_activity += meters_driven\n",
    "\n",
    "                \n",
    "                #Meters driven since last timestamp\n",
    "                speed_kmh = (meters_driven/seconds_gone)*3.6\n",
    "\n",
    "                #Add the speed to a list for entire day\n",
    "                self.stats.day_speeds.append(speed_kmh)\n",
    "\n",
    "                #Add the distance (km) between the two timestamps \n",
    "                self.stats.day_dists.append(meters_driven/1000)\n",
    "\n",
    "                #Add the timestamp for the two above values\n",
    "                self.stats.day_times.append(current_pos.timestamp)\n",
    "\n",
    "                #Compute vectors. This is a lot of code, maybe create some function?\n",
    "                #Create vector for computing reverse of vehicle\n",
    "                vector_start = current_pos.timestamp-timedelta(seconds=criterias.seconds_for_vector)\n",
    "                index_start_vector = 0\n",
    "                for j, ts in enumerate(self.stats.day_times):\n",
    "                    if ts >= vector_start:\n",
    "                        index_start_vector = j\n",
    "                        break\n",
    "                \n",
    "                current_vector = [self.stats.all_positions[i].lat-self.stats.all_positions[i-3].lat,\n",
    "                                    self.stats.all_positions[i].lon-self.stats.all_positions[i-3].lon]\n",
    "                prev_vector = [self.stats.all_positions[index_start_vector].lat-self.stats.all_positions[index_start_vector-3].lat,\n",
    "                                self.stats.all_positions[index_start_vector].lon-self.stats.all_positions[index_start_vector-3].lon]\n",
    "\n",
    "                current_vector_norm = current_vector/np.linalg.norm(current_vector)\n",
    "                prev_vector_norm = prev_vector/np.linalg.norm(prev_vector)\n",
    "                inner_product = np.inner(current_vector_norm,prev_vector_norm)\n",
    "                self.stats.inner_prods.append(inner_product)\n",
    "            \n",
    "                #Check if we are currently in a loading or dumping working area\n",
    "                #If yes, we update value\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                # currently_in_load_working_area = False\n",
    "                # for coord in self.load_cluster_centers:\n",
    "                #     if geopy.distance.geodesic(coord, (current_pos.lat, current_pos.lon)).m < criterias.meters_from_area:\n",
    "                #         currently_in_load_working_area = True\n",
    "                \n",
    "                # currently_in_dump_working_area = False\n",
    "                # for coord in self.dump_cluster_centers:\n",
    "                #     if geopy.distance.geodesic(coord, (current_pos.lat, current_pos.lon)).m < criterias.meters_from_area:\n",
    "                #         currently_in_dump_working_area = True\n",
    "\n",
    "                # #Could be interesting to mark where we enter and exit load and dump zones. Can be done with this code.\n",
    "                # #Can be used for plotting, see notebooks for examples    \n",
    "                # if not in_load_working_area and currently_in_load_working_area:\n",
    "                #     self.entering_load_working_area.append(current_pos.timestamp)\n",
    "                #     in_load_working_area = True\n",
    "                # elif in_load_working_area and not currently_in_load_working_area:\n",
    "                #     self.exiting_load_working_area.append(current_pos.timestamp)\n",
    "                #     in_load_working_area = False\n",
    "\n",
    "                # if not in_dump_working_area and currently_in_dump_working_area:\n",
    "                #     self.entering_dump_working_area.append(current_pos.timestamp)\n",
    "                #     in_dump_working_area = True\n",
    "                # elif in_dump_working_area and not currently_in_dump_working_area:\n",
    "                #     self.exiting_dump_working_area.append(current_pos.timestamp)\n",
    "                #     in_dump_working_area = False\n",
    "\n",
    "                # #Logic for predicting loading point\n",
    "                # if speed_kmh < criterias.speed_limit and meters_since_last_activity>criterias.meters_since_last_activity:\n",
    "                #     if predicting_load:\n",
    "                #         if in_load_working_area:\n",
    "                #             last_min_start = current_pos.timestamp-timedelta(minutes=criterias.minutes_load)\n",
    "                #             index_start_minute = 0\n",
    "                    #         for i, ts in enumerate(self.stats.day_times):\n",
    "                    #             if ts >= last_min_start:\n",
    "                    #                 index_start_minute = i\n",
    "                    #                 break\n",
    "                    #         sum_over_last_minute = np.sum(self.stats.day_speeds[index_start_minute:])\n",
    "                            \n",
    "                    #         if sum_over_last_minute < criterias.max_sum_last_x_minutes_load:\n",
    "                    #             self.predicted.load.points.append((current_pos.lat, current_pos.lon))\n",
    "                    #             self.predicted.load.times.append(current_pos.timestamp)\n",
    "                                \n",
    "                    #             #Have now predicted a load, next dump\n",
    "                    #             predicting_load = False\n",
    "                    #             meters_since_last_activity = 0\n",
    "\n",
    "                    # else: #Predicting dump\n",
    "                    #     if in_dump_working_area:\n",
    "                    #         last_min_start = current_pos.timestamp-timedelta(minutes=criterias.minutes_dump)\n",
    "                    #         index_start_minute = 0\n",
    "                    #         for i, ts in enumerate(self.stats.day_times):\n",
    "                    #             if ts >= last_min_start:\n",
    "                    #                 index_start_minute = i\n",
    "                    #                 break\n",
    "                    #         sum_over_last_minute = np.sum(self.stats.day_speeds[index_start_minute:])\n",
    "                            \n",
    "                    #         if sum_over_last_minute < criterias.max_sum_last_x_minutes_dump and self.stats.inner_prods[-1] < criterias.inner_prod_threshold: #Not ideal, want to instead pick the best among times, not just the first viable option, but difficult with live tracking.\n",
    "                    #             self.predicted.dump.points.append((current_pos.lat, current_pos.lon))\n",
    "                    #             self.predicted.dump.times.append(current_pos.timestamp)\n",
    "                                \n",
    "                    #             #Have now predicted a dump, next load\n",
    "                    #             predicting_load = True\n",
    "                    #             meters_since_last_activity = 0\n",
    "\n",
    "    def get_df_with_ml_data(self):\n",
    "        load_times_set = set(self.stats.load.times)\n",
    "        dump_times_set = set(self.stats.dump.times)\n",
    "        #  create Dataframe with the given variables\n",
    "        df = pd.DataFrame({\n",
    "            \"DateTime\": self.stats.day_times,\n",
    "            \"Time_from_start\": [(time - self.stats.day_times[0]).min for time in self.stats.day_times],\n",
    "            \"Speed\": self.stats.day_speeds,\n",
    "            \"Inner_products\": self.stats.inner_prods,\n",
    "            \"Load\": [1 if time in load_times_set else 0 for time in self.stats.day_times],\n",
    "            \"Dump\": [1 if time in dump_times_set else 0 for time in self.stats.day_times]\n",
    "        })\n",
    "\n",
    "        \n",
    "        return df\n",
    "        # df.to_csv(f'data/ml_model_data/time_speed_{self.machine.machine_id}.csv', index=False, sep=',')\n",
    "\n",
    "        # # Save units to csv file\n",
    "        # units = pd.DataFrame({\n",
    "        #     \"DateTime\": [\"Datetime\"],\n",
    "        #     \"Time_from_start\": [\"Minutes\"],\n",
    "        #     \"Speed\": [\"km/h\"],\n",
    "        #     \"Inner_products\": [\"-\"],\n",
    "        #     \"Load\": [\"-\"],\n",
    "        #     \"Dump\": [\"-\"]\n",
    "        # })\n",
    "        # units.to_csv(f'data/ml_model_data/units_{self.machine.machine_id}.csv', index=False, sep=',')\n",
    "\n",
    "    \n",
    "    def time_plot(self):\n",
    "\n",
    "        # Plots, not wanted when a lot of data\n",
    "        # Create subplots with 2 rows and 1 column\n",
    "        fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.1)\n",
    "\n",
    "        # Add the first line plot to the first subplot\n",
    "        \n",
    "        fig.add_trace(go.Scatter(x=self.stats.day_times, y=self.stats.day_speeds, mode='lines', name='Speed'), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=self.stats.load.times, y=[0 for a in self.stats.load.times], mode='markers', marker=dict(symbol='cross', size=10, color='red'), name='Load actual'), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=self.predicted.load.times, y=[0 for p in self.predicted.load.times], mode='markers', marker=dict(symbol='star', size=10, color='red'), name='Load predicted'), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=self.stats.dump.times, y=[0 for a in self.stats.dump.times], mode='markers', marker=dict(symbol='cross', size=10, color='green'), name='Dump actual'), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=self.predicted.dump.times, y=[0 for p in self.predicted.dump.times], mode='markers', marker=dict(symbol='star', size=10, color='green'), name='Dump predicted'), row=1, col=1)\n",
    "\n",
    "        # Add the second line plot to the second subplot\n",
    "        fig.add_trace(go.Scatter(x=self.stats.day_times, y=np.cumsum(self.stats.day_dists), mode='lines', name='Cumulative distance'), row=2, col=1)\n",
    "        fig.add_trace(go.Scatter(x=self.stats.load.times, y=[0 for a in self.stats.load.times], mode='markers', marker=dict(symbol='cross', size=10, color='red'), name='Load actual'), row=2, col=1)\n",
    "        fig.add_trace(go.Scatter(x=self.predicted.load.times, y=[0 for p in self.predicted.load.times], mode='markers', marker=dict(symbol='star', size=10, color='red'), name='Load predicted'), row=2, col=1)\n",
    "        fig.add_trace(go.Scatter(x=self.stats.dump.times, y=[0 for a in self.stats.dump.times], mode='markers', marker=dict(symbol='cross', size=10, color='green'), name='Dump actual'), row=2, col=1)\n",
    "        fig.add_trace(go.Scatter(x=self.predicted.dump.times, y=[0 for p in self.predicted.dump.times], mode='markers', marker=dict(symbol='star', size=10, color='green'), name='Dump predicted'), row=2, col=1)\n",
    "\n",
    "        # Add the third line plot\n",
    "        fig.add_trace(go.Scatter(x=self.stats.day_times, y=self.stats.inner_prods, mode='lines', name='Inner product of vectors'), row=3, col=1)\n",
    "\n",
    "        # Update layout settings for both subplots\n",
    "        fig.update_layout(title=str('Subplots of Speeds and cumulative distance, machine_id: '+ str(self.machine.machine_id)),\n",
    "                        xaxis_title='Timestamp',\n",
    "                        showlegend=True)\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "    def gantt_plot(self):\n",
    "\n",
    "        #Actual trips\n",
    "        all_trips_for_machine = self.machine.trips\n",
    "        start_end_each_trip_dict_actual = [dict(Start=trips.start_date, End=trips.end_date, Load=trips.load, Dist=trips.length, Id=trips.trip_id) for trips in all_trips_for_machine]\n",
    "        df_pyplot_actual = pd.DataFrame(start_end_each_trip_dict_actual)\n",
    "        \n",
    "        \n",
    "        #Predicted trips\n",
    "        start_end_each_trip_dict_predicted = [dict(Start=self.predicted.load.times[i], End=self.predicted.load.times[i+1]) for i in range(len(self.predicted.load.times)-1)]\n",
    "        df_pyplot_predicted = pd.DataFrame(start_end_each_trip_dict_predicted)\n",
    "\n",
    "        fig = px.timeline(df_pyplot_actual, x_start=\"Start\", x_end=\"End\", custom_data=[\"Load\", \"Dist\", \"Id\"])\n",
    "        fig.update_traces(\n",
    "            hovertemplate=\"<br>\".join([\n",
    "                \"Start: %{base}\",\n",
    "                \"End: %{x}\",\n",
    "                \"Load: %{customdata[0]}\",\n",
    "                \"Distance: %{customdata[1]}\",\n",
    "                \"ID: %{customdata[2]}\"\n",
    "            ])\n",
    "        )\n",
    "        fig.update_yaxes(autorange=\"reversed\") # otherwise tasks are listed from the bottom up\n",
    "        fig.show()\n",
    "\n",
    "        fig = px.timeline(df_pyplot_predicted, x_start=\"Start\", x_end=\"End\")\n",
    "        fig.update_traces(\n",
    "            hovertemplate=\"<br>\".join([\n",
    "                \"Start: %{base}\",\n",
    "                \"End: %{x}\"\n",
    "            ])\n",
    "        )\n",
    "        fig.update_yaxes(autorange=\"reversed\") # otherwise tasks are listed from the bottom up\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n"
     ]
    }
   ],
   "source": [
    "day = '04-27-2023' #DD-MM-YYYY\n",
    "machine_type = 'Truck'\n",
    "#Loading gps data for selected day and day before\n",
    "trip = dataloader.TripsLoader(day)\n",
    "\n",
    "#Use previous day data to create clustering\n",
    "\n",
    "# Convert the date string to a datetime object\n",
    "# date_obj = datetime.strptime(day, \"%m-%d-%Y\")\n",
    "# # Subtract one day using timedelta\n",
    "# new_date = date_obj - timedelta(days=1)\n",
    "# # Format the new date back to the desired format\n",
    "# day_before = new_date.strftime(\"%m-%d-%Y\")\n",
    "\n",
    "# trip_day_before = dataloader.TripsLoader(day_before)\n",
    "# # Get the coordinates of the cluster centers\n",
    "# load_cluster_centers, dump_cluster_centers = generate_load_dump_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trip._machines.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "days = [csv_file.split(\".csv\")[0] for csv_file in os.listdir(\"data/GPSData/trips\")]\n",
    "machine_type = \"Truck\"\n",
    "\n",
    "def save_dfs_with_ml_data() -> None:\n",
    "    \"\"\"\n",
    "    Get all the wanted data (speed, vector) for all vehicles at all days\n",
    "    \"\"\"\n",
    "    df_training = pd.DataFrame()\n",
    "    for day in days[:1]:\n",
    "        df_each_day = pd.read_csv(f\"data/GPSData/tripsInfo/{day}.csv\")\n",
    "        df_each_day = df_each_day[df_each_day[\"MachineType\"] == machine_type]  # get trucks only\n",
    "        for idx, unique_vehicle in enumerate(df_each_day[\"DumperMachineNumber\"].unique()):\n",
    "            trip = dataloader.TripsLoader(day)\n",
    "            machine_of_interest = trip._machines[unique_vehicle]\n",
    "            automated_for_given_machine = automated_load_dump_for_machine(machine_of_interest)\n",
    "            automated_for_given_machine.predict()\n",
    "            # automated_for_given_machine.time_plot()\n",
    "            # automated_for_given_machine.gantt_plot()\n",
    "            df_vehicle = automated_for_given_machine.get_df_with_ml_data()\n",
    "            df_training = pd.concat([df_training, df_vehicle], axis=0)\n",
    "            # break   # we only want one vehicle for now\n",
    "    df_training.to_csv(\"data/ml_model_data/1_day_all_trucks.csv\", sep=',', index=False)\n",
    "    \n",
    "\n",
    "save_dfs_with_ml_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo\n",
    "#   Create a summary report on trips and similar\n",
    "#       Could be plots, metrics and statistics\n",
    "#   Improve prediction\n",
    "#   Classify type of errors, some are due to lack of gps signal, will be difficult to fix\n",
    "#   Improve vector creation, maybe current minus 30 seconds, instead of fixed 30 timesteps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
