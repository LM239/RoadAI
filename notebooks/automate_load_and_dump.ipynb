{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm to detect both load and dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader\n",
    "from pydantic import BaseModel\n",
    "import typing\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import geopy.distance\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from schemas import Machine, Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = '04-06-2022' #DD-MM-YYYY\n",
    "machine_type = 'Truck'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class setting parameters determining when we predict load/dump\n",
    "\n",
    "class criteria(BaseModel):\n",
    "    optimal_K: int = 50                     # Nb of clusters for work areas\n",
    "    meters_from_area: int = 30              # Radius from a cluster center\n",
    "    nb_of_positions_for_vector: int = 30    # Historical \"length\" of vector, make it more stable?\n",
    "    \n",
    "    speed_limit: int = 20                   # Cannot be loading or dumping if speed higher than this\n",
    "    meters_since_last_activity: int = 500   # Meters driven since last load/dump\n",
    "    \n",
    "    minutes_load: int = 3                   # Look at distance driven last x minutes before a possible load\n",
    "    max_sum_last_x_minutes_load: int = 1000 # Max meters driven during the last x minutes\n",
    "    minutes_dump: int = 1                   # Look at distance driven last x minutes before a possible load\n",
    "    max_sum_last_x_minutes_dump: int = 1000 # Max meters driven during the last x minutes\n",
    "    \n",
    "    inner_prod_threshold: float = 0.80      # A threshold to pick up possible reversal\n",
    "\n",
    "criterias = criteria()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading gps data for selected day and day before\n",
    "trip = dataloader.TripsLoader(day)\n",
    "\n",
    "#Use previous day data to create clustering\n",
    "\n",
    "# Convert the date string to a datetime object\n",
    "date_obj = datetime.strptime(day, \"%m-%d-%Y\")\n",
    "# Subtract one day using timedelta\n",
    "new_date = date_obj - timedelta(days=1)\n",
    "# Format the new date back to the desired format\n",
    "day_before = new_date.strftime(\"%m-%d-%Y\")\n",
    "\n",
    "trip_day_before = dataloader.TripsLoader(day_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_load_dump_clusters():\n",
    "    all_load_positions_for_day_before = []\n",
    "    all_dump_positions_for_day_before = []\n",
    "    for machine_number in trip_day_before._machines.keys():\n",
    "        temp_machine = trip_day_before._machines[machine_number]\n",
    "        if temp_machine.machine_type == machine_type:\n",
    "            all_load_positions_for_day_before.append([trip.load_latlon for trip in temp_machine.trips])\n",
    "            all_dump_positions_for_day_before.append([trip.dump_latlon for trip in temp_machine.trips])\n",
    "\n",
    "    all_load_positions_for_day_before =  [item for sublist in all_load_positions_for_day_before for item in sublist]\n",
    "    all_dump_positions_for_day_before = [item for sublist in all_dump_positions_for_day_before for item in sublist]\n",
    "\n",
    "    # Assuming 'coordinates' is list of tuples [(lat1, lon1), (lat2, lon2), ...]\n",
    "    load_coordinates_array = np.array(all_load_positions_for_day_before)\n",
    "    dump_coordinates_array = np.array(all_dump_positions_for_day_before)\n",
    "\n",
    "    # Fit the KMeans model with the optimal K value\n",
    "    load_kmeans = KMeans(n_clusters=criterias.optimal_K, random_state=42, n_init='auto')\n",
    "    dump_kmeans = KMeans(n_clusters=criterias.optimal_K, random_state=42, n_init='auto')\n",
    "    load_kmeans.fit(load_coordinates_array)\n",
    "    dump_kmeans.fit(dump_coordinates_array)\n",
    "\n",
    "    # Get the coordinates of the cluster centers for the optimal K value\n",
    "    load_cluster_centers = load_kmeans.cluster_centers_\n",
    "    dump_cluster_centers = dump_kmeans.cluster_centers_\n",
    "\n",
    "    return load_cluster_centers, dump_cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coordinates of the cluster centers\n",
    "load_cluster_centers, dump_cluster_centers = generate_load_dump_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(load_cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class points_times(BaseModel):\n",
    "    points: list[tuple[float, float]]\n",
    "    times: list[datetime]\n",
    "\n",
    "class predicted_load_dump(BaseModel):\n",
    "    load: points_times\n",
    "    dump: points_times\n",
    "\n",
    "class stats(BaseModel): #Represents actual data\n",
    "    all_positions: list[Position] # Positions recorded during a day\n",
    "    load: points_times            # Load points and times\n",
    "    dump: points_times            # Dump points and times\n",
    "    day_speeds: list[float]       # Speeds\n",
    "    day_dists: list[float]        # Distances between each recording\n",
    "    day_times: list[datetime]     # Timestamp for two above lists\n",
    "    inner_prods: list[float]      # Inner product of consecutive normalized driving vectors\n",
    "\n",
    "class automated_load_dump_for_machine():\n",
    "\n",
    "    def __init__(self, \n",
    "                 machine_data: Machine, \n",
    "                 load_cluster_centers: typing.Any,# Both should be list[tuple[float, float]], should implement \n",
    "                 dump_cluster_centers: typing.Any) -> None:\n",
    "        \n",
    "        self.machine = machine_data\n",
    "        self.predicted = predicted_load_dump\n",
    "        self.stats = stats\n",
    "\n",
    "\n",
    "        all_pos = [trips.positions for trips in self.machine.trips]\n",
    "        self.stats.all_positions = [item for sublist in all_pos for item in sublist]\n",
    "        self.stats.load.points = [trips.load_latlon for trips in self.machine.trips]\n",
    "        self.stats.load.times = [trips.positions[0].timestamp for trips in self.machine.trips]\n",
    "        self.stats.dump.points = [trips.dump_latlon for trips in self.machine.trips]\n",
    "        \n",
    "        actual_dump_times = []\n",
    "        for t in self.machine.trips:        #Not pretty, because we don't have dump time in trip info by default\n",
    "            temp_dump_laton = t.dump_latlon # Must match latlons\n",
    "            for position in t.positions:\n",
    "                if temp_dump_laton == (position.lat, position.lon):\n",
    "                    actual_dump_times.append(position.timestamp)\n",
    "                    break\n",
    "        self.stats.dump.times = actual_dump_times\n",
    "\n",
    "        self.load_cluster_centers = load_cluster_centers\n",
    "        self.dump_cluster_centers = dump_cluster_centers\n",
    "\n",
    "        #These four lines should be rewritten, to a class or something\n",
    "        self.entering_load_working_area = []\n",
    "        self.exiting_load_working_area = []\n",
    "        self.entering_dump_working_area = []\n",
    "        self.exiting_dump_working_area = []\n",
    "\n",
    "    def predict(self):\n",
    "\n",
    "        #We know first loading because that is when data begins\n",
    "        self.predicted.load.points.append((self.stats.all_positions[0].lat, self.stats.all_positions[0].lon))\n",
    "        self.predicted.load.times.append(self.stats.all_positions[0].timestamp)\n",
    "\n",
    "        #When true, we are predicting load, else dump. Next prediction will be dump, since we have load above\n",
    "        predicting_load = False\n",
    "\n",
    "        #Initialize variables that keep track of whether or not we are in a usual area for loading or dumping\n",
    "        #As given by load and dump clusters and criterias.meters_from_area\n",
    "        in_load_working_area = False #Probably true, since first position is when loading\n",
    "        in_dump_working_area = False #Maybe false, since first position is when loading\n",
    "\n",
    "        #We determine the true value of the two above variables\n",
    "        for coord in self.load_cluster_centers: #But verify with created clusters of load points from day before\n",
    "                if geopy.distance.geodesic(coord, (self.stats.all_positions[0].lat, self.stats.all_positions[0].lon)).m < criterias.meters_from_area:\n",
    "                    in_load_working_area = True\n",
    "\n",
    "        for coord in self.dump_cluster_centers: #But verify with created clusters of load points from day before\n",
    "                if geopy.distance.geodesic(coord, (self.stats.all_positions[0].lat, self.stats.all_positions[0].lon)).m < criterias.meters_from_area:\n",
    "                    in_dump_working_area = True\n",
    "\n",
    "        #We keep track of how many meters we have driven since last dump or load    \n",
    "        meters_since_last_activity = 0\n",
    "        \n",
    "        #We start predicting. Are going to iterate over all positions, from first to last\n",
    "        for i in range(1,len(self.stats.all_positions[1:])):\n",
    "            \n",
    "            current_pos = self.stats.all_positions[i]\n",
    "            prev_pos = self.stats.all_positions[i-1]\n",
    "\n",
    "            #Seconds passed since last timestamp\n",
    "            seconds_gone = (current_pos.timestamp.to_pydatetime()-prev_pos.timestamp.to_pydatetime()).total_seconds()\n",
    "\n",
    "            if seconds_gone > 0:\n",
    "\n",
    "                #Meters driven since last timestamp\n",
    "                meters_driven = geopy.distance.geodesic((current_pos.lat, current_pos.lon), (prev_pos.lat, prev_pos.lon)).m\n",
    "                meters_since_last_activity += meters_driven\n",
    "\n",
    "                #Compute vectors. This is a lot of code, maybe create some function?\n",
    "                if i > 1+criterias.nb_of_positions_for_vector: #Create vector for computing reverse of vehicle\n",
    "                    \n",
    "                    current_vector = [self.stats.all_positions[i].lat-self.stats.all_positions[i-criterias.nb_of_positions_for_vector].lat,\n",
    "                                      self.stats.all_positions[i].lon-self.stats.all_positions[i-criterias.nb_of_positions_for_vector].lon]\n",
    "                    prev_vector = [self.stats.all_positions[i-1].lat-self.stats.all_positions[i-1-criterias.nb_of_positions_for_vector].lat,\n",
    "                                   self.stats.all_positions[i-1].lon-self.stats.all_positions[i-1-criterias.nb_of_positions_for_vector].lon]\n",
    "\n",
    "                    current_vector_norm = current_vector/np.linalg.norm(current_vector)\n",
    "                    prev_vector_norm = prev_vector/np.linalg.norm(prev_vector)\n",
    "                    inner_product = np.inner(current_vector_norm,prev_vector_norm)\n",
    "                    self.stats.inner_prods.append(inner_product)\n",
    "                else: #Temporary solution, maybe sufficient\n",
    "                    self.stats.inner_prods.append(0)\n",
    "\n",
    "                \n",
    "                #Meters driven since last timestamp\n",
    "                speed_kmh = (meters_driven/seconds_gone)*3.6\n",
    "\n",
    "                #Add the speed to a list for entire day\n",
    "                self.stats.day_speeds.append(speed_kmh)\n",
    "\n",
    "                #Add the distance (km) between the two timestamps \n",
    "                self.stats.day_dists.append(meters_driven/1000)\n",
    "\n",
    "                #Add the timestamp for the two above values\n",
    "                self.stats.day_times.append(current_pos.timestamp)\n",
    "                \n",
    "                #Check if we are currently in a loading or dumping working area\n",
    "                #If yes, we update value\n",
    "                currently_in_load_working_area = False\n",
    "                for coord in self.load_cluster_centers:\n",
    "                    if geopy.distance.geodesic(coord, (current_pos.lat, current_pos.lon)).m < criterias.meters_from_area:\n",
    "                        currently_in_load_working_area = True\n",
    "                \n",
    "                currently_in_dump_working_area = False\n",
    "                for coord in self.dump_cluster_centers:\n",
    "                    if geopy.distance.geodesic(coord, (current_pos.lat, current_pos.lon)).m < criterias.meters_from_area:\n",
    "                        currently_in_dump_working_area = True\n",
    "\n",
    "                #Could be interesting to mark where we enter and exit load and dump zones. Can be done with this code.\n",
    "                #Can be used for plotting, see notebooks for examples    \n",
    "                if not in_load_working_area and currently_in_load_working_area:\n",
    "                    self.entering_load_working_area.append(current_pos.timestamp)\n",
    "                    in_load_working_area = True\n",
    "                elif in_load_working_area and not currently_in_load_working_area:\n",
    "                    self.exiting_load_working_area.append(current_pos.timestamp)\n",
    "                    in_load_working_area = False\n",
    "\n",
    "                if not in_dump_working_area and currently_in_dump_working_area:\n",
    "                    self.entering_dump_working_area.append(current_pos.timestamp)\n",
    "                    in_dump_working_area = True\n",
    "                elif in_dump_working_area and not currently_in_dump_working_area:\n",
    "                    self.exiting_dump_working_area.append(current_pos.timestamp)\n",
    "                    in_dump_working_area = False\n",
    "\n",
    "                #Logic for predicting loading point\n",
    "                if speed_kmh < criterias.speed_limit and meters_since_last_activity>criterias.meters_since_last_activity:\n",
    "                    if predicting_load:\n",
    "                        if in_load_working_area:\n",
    "                            last_min_start = current_pos.timestamp-timedelta(minutes=criterias.minutes_load)\n",
    "                            index_start_minute = 0\n",
    "                            for i, ts in enumerate(self.stats.day_times):\n",
    "                                if ts >= last_min_start:\n",
    "                                    index_start_minute = i\n",
    "                                    break\n",
    "                            sum_over_last_minute = np.sum(self.stats.day_speeds[index_start_minute:])\n",
    "                            \n",
    "                            if sum_over_last_minute < criterias.max_sum_last_x_minutes_load:\n",
    "                                self.predicted.load.points.append((current_pos.lat, current_pos.lon))\n",
    "                                self.predicted.load.times.append(current_pos.timestamp)\n",
    "                                \n",
    "                                #Have now predicted a load, next dump\n",
    "                                predicting_load = False\n",
    "                                meters_since_last_activity = 0\n",
    "\n",
    "                    else: #Predicting dump\n",
    "                        if in_dump_working_area:\n",
    "                            last_min_start = current_pos.timestamp-timedelta(minutes=criterias.minutes_dump)\n",
    "                            index_start_minute = 0\n",
    "                            for i, ts in enumerate(self.stats.day_times):\n",
    "                                if ts >= last_min_start:\n",
    "                                    index_start_minute = i\n",
    "                                    break\n",
    "                            sum_over_last_minute = np.sum(self.stats.day_speeds[index_start_minute:])\n",
    "                            \n",
    "                            if sum_over_last_minute < criterias.max_sum_last_x_minutes_dump and self.stats.inner_prods[-1] < criterias.inner_prod_threshold: #Not ideal, want to instead pick the best among times, not just the first viable option, but difficult with live tracking.\n",
    "                                self.predicted.dump.points.append((current_pos.lat, current_pos.lon))\n",
    "                                self.predicted.dump.times.append(current_pos.timestamp)\n",
    "                                \n",
    "                                #Have now predicted a dump, next load\n",
    "                                predicting_load = True\n",
    "                                meters_since_last_activity = 0\n",
    "\n",
    "    def time_plot(self):\n",
    "\n",
    "        # Plots, not wanted when a lot of data\n",
    "        # Create subplots with 2 rows and 1 column\n",
    "        fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.1)\n",
    "\n",
    "        # Add the first line plot to the first subplot\n",
    "        fig.add_trace(go.Scatter(x=self.stats.day_times, y=self.stats.day_speeds, mode='lines', name='Speed'), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=self.stats.load.times, y=[0 for a in self.stats.load.times], mode='markers', marker=dict(symbol='cross', size=10, color='red'), name='Load actual'), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=self.predicted.load.times, y=[0 for p in self.predicted.load.times], mode='markers', marker=dict(symbol='star', size=10, color='red'), name='Load predicted'), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=self.stats.dump.times, y=[0 for a in self.stats.dump.times], mode='markers', marker=dict(symbol='cross', size=10, color='green'), name='Dump actual'), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=self.predicted.dump.times, y=[0 for p in self.predicted.dump.times], mode='markers', marker=dict(symbol='star', size=10, color='green'), name='Dump predicted'), row=1, col=1)\n",
    "\n",
    "        # Add the second line plot to the second subplot\n",
    "        fig.add_trace(go.Scatter(x=self.stats.day_times, y=np.cumsum(self.stats.day_dists), mode='lines', name='Cumulative distance'), row=2, col=1)\n",
    "        fig.add_trace(go.Scatter(x=self.stats.load.times, y=[0 for a in self.stats.load.times], mode='markers', marker=dict(symbol='cross', size=10, color='red'), name='Load actual'), row=2, col=1)\n",
    "        fig.add_trace(go.Scatter(x=self.predicted.load.times, y=[0 for p in self.predicted.load.times], mode='markers', marker=dict(symbol='star', size=10, color='red'), name='Load predicted'), row=2, col=1)\n",
    "        fig.add_trace(go.Scatter(x=self.stats.dump.times, y=[0 for a in self.stats.dump.times], mode='markers', marker=dict(symbol='cross', size=10, color='green'), name='Dump actual'), row=1, col=2)\n",
    "        fig.add_trace(go.Scatter(x=self.predicted.dump.times, y=[0 for p in self.predicted.dump.times], mode='markers', marker=dict(symbol='star', size=10, color='green'), name='Dump predicted'), row=2, col=1)\n",
    "\n",
    "        # Add the third line plot\n",
    "        fig.add_trace(go.Scatter(x=self.stats.day_times, y=self.stats.inner_prods, mode='lines', name='Inner product of vectors'), row=3, col=1)\n",
    "\n",
    "        # Update layout settings for both subplots\n",
    "        fig.update_layout(title=str('Subplots of Speeds and cumulative distance, machine_id: '+ str(self.machine.machine_id)),\n",
    "                        xaxis_title='Timestamp',\n",
    "                        showlegend=True)\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "    def gantt_plot(self):\n",
    "\n",
    "        #Actual trips\n",
    "        all_trips_for_machine = self.machine.trips\n",
    "        start_end_each_trip_dict_actual = [dict(Start=trips.start_date, End=trips.end_date, Load=trips.load, Dist=trips.length, Id=trips.trip_id) for trips in all_trips_for_machine]\n",
    "        df_pyplot_actual = pd.DataFrame(start_end_each_trip_dict_actual)\n",
    "        \n",
    "        \n",
    "        #Predicted trips\n",
    "        start_end_each_trip_dict_predicted = [dict(Start=self.predicted.load.times[i], End=self.predicted.load.times[i+1]) for i in range(len(self.predicted.load.times)-1)]\n",
    "        df_pyplot_predicted = pd.DataFrame(start_end_each_trip_dict_predicted)\n",
    "\n",
    "        fig = px.timeline(df_pyplot_actual, x_start=\"Start\", x_end=\"End\", custom_data=[\"Load\", \"Dist\", \"Id\"])\n",
    "        fig.update_traces(\n",
    "            hovertemplate=\"<br>\".join([\n",
    "                \"Start: %{base}\",\n",
    "                \"End: %{x}\",\n",
    "                \"Load: %{customdata[0]}\",\n",
    "                \"Distance: %{customdata[1]}\",\n",
    "                \"ID: %{customdata[2]}\"\n",
    "            ])\n",
    "        )\n",
    "        fig.update_yaxes(autorange=\"reversed\") # otherwise tasks are listed from the bottom up\n",
    "        fig.show()\n",
    "\n",
    "        fig = px.timeline(df_pyplot_predicted, x_start=\"Start\", x_end=\"End\")\n",
    "        fig.update_traces(\n",
    "            hovertemplate=\"<br>\".join([\n",
    "                \"Start: %{base}\",\n",
    "                \"End: %{x}\"\n",
    "            ])\n",
    "        )\n",
    "        fig.update_yaxes(autorange=\"reversed\") # otherwise tasks are listed from the bottom up\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "load",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m some_machine \u001b[39m=\u001b[39m trip\u001b[39m.\u001b[39m_machines[machine_nb]\n\u001b[1;32m      4\u001b[0m \u001b[39mif\u001b[39;00m some_machine\u001b[39m.\u001b[39mmachine_type \u001b[39m==\u001b[39m machine_type:\n\u001b[0;32m----> 6\u001b[0m     automated_for_given_machine \u001b[39m=\u001b[39m automated_load_dump_for_machine(some_machine, load_cluster_centers, dump_cluster_centers)\n\u001b[1;32m      7\u001b[0m     automated_for_given_machine\u001b[39m.\u001b[39mpredict()\n\u001b[1;32m      8\u001b[0m     automated_for_given_machine\u001b[39m.\u001b[39mtime_plot()\n",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m, in \u001b[0;36mautomated_load_dump_for_machine.__init__\u001b[0;34m(self, machine_data, load_cluster_centers, dump_cluster_centers)\u001b[0m\n\u001b[1;32m     30\u001b[0m all_pos \u001b[39m=\u001b[39m [trips\u001b[39m.\u001b[39mpositions \u001b[39mfor\u001b[39;00m trips \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmachine\u001b[39m.\u001b[39mtrips]\n\u001b[1;32m     31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstats\u001b[39m.\u001b[39mall_positions \u001b[39m=\u001b[39m [item \u001b[39mfor\u001b[39;00m sublist \u001b[39min\u001b[39;00m all_pos \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m sublist]\n\u001b[0;32m---> 32\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstats\u001b[39m.\u001b[39;49mload\u001b[39m.\u001b[39mpoints \u001b[39m=\u001b[39m [trips\u001b[39m.\u001b[39mload_latlon \u001b[39mfor\u001b[39;00m trips \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmachine\u001b[39m.\u001b[39mtrips]\n\u001b[1;32m     33\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstats\u001b[39m.\u001b[39mload\u001b[39m.\u001b[39mtimes \u001b[39m=\u001b[39m [trips\u001b[39m.\u001b[39mpositions[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtimestamp \u001b[39mfor\u001b[39;00m trips \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmachine\u001b[39m.\u001b[39mtrips]\n\u001b[1;32m     34\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstats\u001b[39m.\u001b[39mdump\u001b[39m.\u001b[39mpoints \u001b[39m=\u001b[39m [trips\u001b[39m.\u001b[39mdump_latlon \u001b[39mfor\u001b[39;00m trips \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmachine\u001b[39m.\u001b[39mtrips]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:205\u001b[0m, in \u001b[0;36mModelMetaclass.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[39mif\u001b[39;00m rebuilt_validator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m             \u001b[39m# In this case, a validator was built, and so `__pydantic_core_schema__` should now be set\u001b[39;00m\n\u001b[1;32m    204\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m__pydantic_core_schema__\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 205\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(item)\n",
      "\u001b[0;31mAttributeError\u001b[0m: load"
     ]
    }
   ],
   "source": [
    "for machine_nb in tqdm(trip._machines.keys()):\n",
    "    some_machine = trip._machines[machine_nb]\n",
    "    \n",
    "    if some_machine.machine_type == machine_type:\n",
    "\n",
    "        automated_for_given_machine = automated_load_dump_for_machine(some_machine, load_cluster_centers, dump_cluster_centers)\n",
    "        automated_for_given_machine.predict()\n",
    "        automated_for_given_machine.time_plot()\n",
    "        automated_for_given_machine.gantt_plot()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for machine_nb in tqdm(trip._machines.keys()):\n",
    "    some_machine = trip._machines[machine_nb]\n",
    "    \n",
    "    if some_machine.machine_type == machine_type:\n",
    "        \n",
    "        predicted_load_points = []\n",
    "        predicted_load_times = []\n",
    "        predicted_dump_points = []\n",
    "        predicted_dump_times = []\n",
    "        \n",
    "        day_speeds = []\n",
    "        day_dists = []\n",
    "        day_times = []\n",
    "\n",
    "        list_of_inner_prods = []\n",
    "\n",
    "        all_pos = [trips.positions for trips in some_machine.trips]\n",
    "        all_pos = [item for sublist in all_pos for item in sublist]\n",
    "\n",
    "        #Now have all positions with timestamps in one liste\n",
    "        #Can get actual points and times of loading\n",
    "        actual_load_points = [trips.load_latlon for trips in some_machine.trips]\n",
    "        actual_load_times = [trips.positions[0].timestamp for trips in some_machine.trips]\n",
    "        actual_dump_points = [trips.dump_latlon for trips in some_machine.trips]\n",
    "        actual_dump_times = []\n",
    "        for t in some_machine.trips: #Not pretty\n",
    "            temp_dump_laton = t.dump_latlon\n",
    "            for position in t.positions:\n",
    "                if temp_dump_laton == (position.lat, position.lon):\n",
    "                    actual_dump_times.append(position.timestamp)\n",
    "                    break\n",
    "\n",
    "        #First registered point is necessarily loading\n",
    "        predicted_load_points.append((all_pos[0].lat,all_pos[0].lon))\n",
    "        predicted_load_times.append(all_pos[0].timestamp)\n",
    "        predicting_load = False #False meaning that next next prediction will be dump\n",
    "\n",
    "        in_load_working_area = False #Probably true, since first position is when loading\n",
    "        in_dump_working_area = False #Maybe false, since first position is when loading\n",
    "\n",
    "        for coord in load_cluster_centers: #But verify with created clusters of load points from day before\n",
    "                if geopy.distance.geodesic(coord, (all_pos[0].lat, all_pos[0].lon)).m < 30:\n",
    "                    in_load_working_area = True\n",
    "\n",
    "        for coord in dump_cluster_centers: #But verify with created clusters of load points from day before\n",
    "                if geopy.distance.geodesic(coord, (all_pos[0].lat, all_pos[0].lon)).m < 30:\n",
    "                    in_dump_working_area = True\n",
    "            \n",
    "        meters_since_last_activity = 0\n",
    "\n",
    "        for i in range(1,len(all_pos[1:])):\n",
    "            \n",
    "            current_pos = all_pos[i]\n",
    "            prev_pos = all_pos[i-1]\n",
    "            \n",
    "            #Seconds passed since last timestamp\n",
    "            seconds_gone = (current_pos.timestamp.to_pydatetime()-prev_pos.timestamp.to_pydatetime()).total_seconds()\n",
    "\n",
    "            if seconds_gone > 0:\n",
    "\n",
    "                #Meters driven since last timestamp\n",
    "                meters_driven = geopy.distance.geodesic((current_pos.lat, current_pos.lon), (prev_pos.lat, prev_pos.lon)).m\n",
    "                meters_since_last_activity += meters_driven\n",
    "\n",
    "                if i > 1+criterias.nb_of_positions_for_vector: #Create vector for computing reverse of vehicle\n",
    "                    \n",
    "                    current_vector = [all_pos[i].lat-all_pos[i-criterias.nb_of_positions_for_vector].lat,all_pos[i].lon-all_pos[i-criterias.nb_of_positions_for_vector].lon]\n",
    "                    prev_vector = [all_pos[i-1].lat-all_pos[i-1-criterias.nb_of_positions_for_vector].lat,all_pos[i-1].lon-all_pos[i-1-criterias.nb_of_positions_for_vector].lon]\n",
    "\n",
    "                    current_vector_norm = current_vector/np.linalg.norm(current_vector)\n",
    "                    prev_vector_norm = prev_vector/np.linalg.norm(prev_vector)\n",
    "                    inner_product = np.inner(current_vector_norm,prev_vector_norm)\n",
    "                    list_of_inner_prods.append(inner_product)\n",
    "                else:\n",
    "                    list_of_inner_prods.append(0)\n",
    "                \n",
    "                speed_kmh = (meters_driven/seconds_gone)*3.6\n",
    "                day_speeds.append(speed_kmh)\n",
    "                day_dists.append(meters_driven/1000)\n",
    "                day_times.append(current_pos.timestamp)\n",
    "\n",
    "                currently_in_load_working_area = False\n",
    "                for coord in load_cluster_centers:\n",
    "                    if geopy.distance.geodesic(coord, (current_pos.lat, current_pos.lon)).m < criterias.meters_from_area:\n",
    "                        currently_in_load_working_area = True\n",
    "                \n",
    "                currently_in_dump_working_area = False\n",
    "                for coord in dump_cluster_centers:\n",
    "                    if geopy.distance.geodesic(coord, (current_pos.lat, current_pos.lon)).m < criterias.meters_from_area:\n",
    "                        currently_in_dump_working_area = True\n",
    "                    \n",
    "\n",
    "                #Two below blocks not really necessary if we are not to plot when the leave or enter work area    \n",
    "                if not in_load_working_area and currently_in_load_working_area:\n",
    "                    #entering_working_area.append(current_pos.timestamp)\n",
    "                    in_load_working_area = True\n",
    "                elif in_load_working_area and not currently_in_load_working_area:\n",
    "                    #exiting_working_area.append(current_pos.timestamp)\n",
    "                    in_load_working_area = False\n",
    "\n",
    "                if not in_dump_working_area and currently_in_dump_working_area:\n",
    "                    #entering_working_area.append(current_pos.timestamp)\n",
    "                    in_dump_working_area = True\n",
    "                elif in_dump_working_area and not currently_in_dump_working_area:\n",
    "                    #exiting_working_area.append(current_pos.timestamp)\n",
    "                    in_dump_working_area = False\n",
    "\n",
    "                #Logic for predicting loading point\n",
    "                if speed_kmh < criterias.speed_limit and meters_since_last_activity>criterias.meters_since_last_activity:\n",
    "                    if predicting_load:\n",
    "                        if in_load_working_area:\n",
    "                            last_min_start = current_pos.timestamp-timedelta(minutes=criterias.minutes_load)\n",
    "                            index_start_minute = 0\n",
    "                            for i, ts in enumerate(day_times):\n",
    "                                if ts >= last_min_start:\n",
    "                                    index_start_minute = i\n",
    "                                    break\n",
    "                            sum_over_last_minute = np.sum(day_speeds[index_start_minute:])\n",
    "                            \n",
    "                            if sum_over_last_minute < criterias.max_sum_last_x_minutes_load:\n",
    "                                predicted_load_points.append((current_pos.lat, current_pos.lon))\n",
    "                                predicted_load_times.append(current_pos.timestamp)\n",
    "                                \n",
    "                                #Have now predicted a load\n",
    "                                predicting_load = False\n",
    "                                meters_since_last_activity = 0\n",
    "\n",
    "                    else: #Predicting dump\n",
    "                        if in_dump_working_area:\n",
    "                            last_min_start = current_pos.timestamp-timedelta(minutes=criterias.minutes_dump)\n",
    "                            index_start_minute = 0\n",
    "                            for i, ts in enumerate(day_times):\n",
    "                                if ts >= last_min_start:\n",
    "                                    index_start_minute = i\n",
    "                                    break\n",
    "                            sum_over_last_minute = np.sum(day_speeds[index_start_minute:])\n",
    "                            \n",
    "                            if sum_over_last_minute < criterias.max_sum_last_x_minutes_dump and list_of_inner_prods[-1] < criterias.inner_prod_threshold: #Not ideal, want to instead pick the best among times, not just the first viable option, but difficult with live tracking.\n",
    "                                predicted_dump_points.append((current_pos.lat, current_pos.lon))\n",
    "                                predicted_dump_times.append(current_pos.timestamp)\n",
    "                                \n",
    "                                #Have now predicted a dump\n",
    "                                predicting_load = True\n",
    "                                meters_since_last_activity = 0\n",
    "\n",
    "                     \n",
    "        \n",
    "                \n",
    "        # Plots, not wanted when a lot of data\n",
    "        # Create subplots with 2 rows and 1 column\n",
    "        fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.1)\n",
    "\n",
    "        # Add the first line plot to the first subplot\n",
    "        fig.add_trace(go.Scatter(x=day_times, y=day_speeds, mode='lines', name='Speed'), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=actual_load_times, y=[0 for a in actual_load_times], mode='markers', marker=dict(symbol='cross', size=10, color='red'), name='Load actual'), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=predicted_load_times, y=[0 for p in predicted_load_times], mode='markers', marker=dict(symbol='star', size=10, color='red'), name='Load predicted'), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=actual_dump_times, y=[0 for a in actual_dump_times], mode='markers', marker=dict(symbol='cross', size=10, color='green'), name='Dump actual'), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=predicted_dump_times, y=[0 for p in predicted_dump_times], mode='markers', marker=dict(symbol='star', size=10, color='green'), name='Dump predicted'), row=1, col=1)\n",
    "\n",
    "        # Add the second line plot to the second subplot\n",
    "        fig.add_trace(go.Scatter(x=day_times, y=np.cumsum(day_dists), mode='lines', name='Cumulative distance'), row=2, col=1)\n",
    "        fig.add_trace(go.Scatter(x=actual_load_times, y=[0 for a in actual_load_times], mode='markers', marker=dict(symbol='cross', size=10, color='red'), name='Load actual'), row=2, col=1)\n",
    "        fig.add_trace(go.Scatter(x=predicted_load_times, y=[0 for p in predicted_load_times], mode='markers', marker=dict(symbol='star', size=10, color='red'), name='Load predicted'), row=2, col=1)\n",
    "        fig.add_trace(go.Scatter(x=actual_dump_times, y=[0 for a in actual_dump_times], mode='markers', marker=dict(symbol='cross', size=10, color='green'), name='Dump actual'), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=predicted_dump_times, y=[0 for p in predicted_dump_times], mode='markers', marker=dict(symbol='star', size=10, color='green'), name='Dump predicted'), row=1, col=1)\n",
    "\n",
    "        # Add the third line plot\n",
    "        fig.add_trace(go.Scatter(x=day_times, y=list_of_inner_prods, mode='lines', name='Inner product of vectors'), row=3, col=1)\n",
    "\n",
    "        # Update layout settings for both subplots\n",
    "        fig.update_layout(title=str('Subplots of Speeds and cumulative distance, machine_id: '+ str(some_machine.machine_id)),\n",
    "                        xaxis_title='Timestamp',\n",
    "                        showlegend=True)\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "        #Make gantt plot for machine on day\n",
    "\n",
    "        #Actual trips\n",
    "        all_trips_for_machine = some_machine.trips\n",
    "        start_end_each_trip_dict_actual = [dict(Start=trips.start_date, End=trips.end_date, Load=trips.load, Dist=trips.length, Id=trips.trip_id) for trips in all_trips_for_machine]\n",
    "        df_pyplot_actual = pd.DataFrame(start_end_each_trip_dict_actual)\n",
    "        \n",
    "        \n",
    "        #Predicted trips\n",
    "        start_end_each_trip_dict_predicted = [dict(Start=predicted_load_times[i], End=predicted_load_times[i+1]) for i in range(len(predicted_load_times)-1)]\n",
    "        df_pyplot_predicted = pd.DataFrame(start_end_each_trip_dict_predicted)\n",
    "\n",
    "        fig = px.timeline(df_pyplot_actual, x_start=\"Start\", x_end=\"End\", custom_data=[\"Load\", \"Dist\", \"Id\"])\n",
    "        fig.update_traces(\n",
    "            hovertemplate=\"<br>\".join([\n",
    "                \"Start: %{base}\",\n",
    "                \"End: %{x}\",\n",
    "                \"Load: %{customdata[0]}\",\n",
    "                \"Distance: %{customdata[1]}\",\n",
    "                \"ID: %{customdata[2]}\"\n",
    "            ])\n",
    "        )\n",
    "        fig.update_yaxes(autorange=\"reversed\") # otherwise tasks are listed from the bottom up\n",
    "        fig.show()\n",
    "\n",
    "        fig = px.timeline(df_pyplot_predicted, x_start=\"Start\", x_end=\"End\")\n",
    "        fig.update_traces(\n",
    "            hovertemplate=\"<br>\".join([\n",
    "                \"Start: %{base}\",\n",
    "                \"End: %{x}\"\n",
    "            ])\n",
    "        )\n",
    "        fig.update_yaxes(autorange=\"reversed\") # otherwise tasks are listed from the bottom up\n",
    "        fig.show()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
