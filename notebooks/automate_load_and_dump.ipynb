{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm to detect both load and dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader\n",
    "from pydantic import BaseModel\n",
    "import typing\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import geopy.distance\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from schemas import Machine, Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = '04-27-2023' #DD-MM-YYYY\n",
    "machine_type = 'Truck'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class setting parameters determining when we predict load/dump\n",
    "\n",
    "class criteria(BaseModel):\n",
    "    optimal_K: int = 50                     # Nb of clusters for work areas\n",
    "    meters_from_area: int = 30              # Radius from a cluster center\n",
    "    seconds_for_vector: int = 10            # \"Length\" of vector used to determine if vehicle is reversing\n",
    "    \n",
    "    speed_limit: int = 30                   # Cannot be loading or dumping if speed higher than this. Don't like such a high number.\n",
    "    meters_since_last_activity: int = 500   # Meters driven since last load/dump\n",
    "    \n",
    "    minutes_load: int = 3                   # Look at distance driven last x minutes before a possible load\n",
    "    max_sum_last_x_minutes_load: int = 1000 # Max meters driven during the last x minutes\n",
    "    minutes_dump: int = 3                   # Look at distance driven last x minutes before a possible load\n",
    "    max_sum_last_x_minutes_dump: int = 1000 # Max meters driven during the last x minutes\n",
    "    \n",
    "    inner_prod_threshold: float = 0.80      # A threshold to pick up possible reversal\n",
    "\n",
    "criterias = criteria()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n",
      "Could not add row, row was type:  <class 'float'>  but expected ndarray.\n"
     ]
    }
   ],
   "source": [
    "day = '04-27-2023' #DD-MM-YYYY\n",
    "machine_type = 'Truck'\n",
    "#Loading gps data for selected day and day before\n",
    "trip = dataloader.TripsLoader(day)\n",
    "\n",
    "#Use previous day data to create clustering\n",
    "\n",
    "# Convert the date string to a datetime object\n",
    "date_obj = datetime.strptime(day, \"%m-%d-%Y\")\n",
    "# Subtract one day using timedelta\n",
    "new_date = date_obj - timedelta(days=1)\n",
    "# Format the new date back to the desired format\n",
    "day_before = new_date.strftime(\"%m-%d-%Y\")\n",
    "\n",
    "trip_day_before = dataloader.TripsLoader(day_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_load_dump_clusters():\n",
    "    all_load_positions_for_day_before = []\n",
    "    all_dump_positions_for_day_before = []\n",
    "    for machine_number in trip_day_before._machines.keys():\n",
    "        temp_machine = trip_day_before._machines[machine_number]\n",
    "        if temp_machine.machine_type == machine_type:\n",
    "            all_load_positions_for_day_before.append([trip.load_latlon for trip in temp_machine.trips])\n",
    "            all_dump_positions_for_day_before.append([trip.dump_latlon for trip in temp_machine.trips])\n",
    "\n",
    "    all_load_positions_for_day_before =  [item for sublist in all_load_positions_for_day_before for item in sublist]\n",
    "    all_dump_positions_for_day_before = [item for sublist in all_dump_positions_for_day_before for item in sublist]\n",
    "\n",
    "    # Assuming 'coordinates' is list of tuples [(lat1, lon1), (lat2, lon2), ...]\n",
    "    load_coordinates_array = np.array(all_load_positions_for_day_before)\n",
    "    dump_coordinates_array = np.array(all_dump_positions_for_day_before)\n",
    "\n",
    "    # Fit the KMeans model with the optimal K value\n",
    "    load_kmeans = KMeans(n_clusters=criterias.optimal_K, random_state=42, n_init='auto')\n",
    "    dump_kmeans = KMeans(n_clusters=criterias.optimal_K, random_state=42, n_init='auto')\n",
    "    load_kmeans.fit(load_coordinates_array)\n",
    "    dump_kmeans.fit(dump_coordinates_array)\n",
    "\n",
    "    # Get the coordinates of the cluster centers for the optimal K value\n",
    "    load_cluster_centers = load_kmeans.cluster_centers_\n",
    "    dump_cluster_centers = dump_kmeans.cluster_centers_\n",
    "\n",
    "    return load_cluster_centers, dump_cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coordinates of the cluster centers\n",
    "load_cluster_centers, dump_cluster_centers = generate_load_dump_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class points_times(BaseModel):\n",
    "    points: list[tuple[float, float]] = []\n",
    "    times: list[datetime] = []\n",
    "\n",
    "class predicted_load_dump(BaseModel):\n",
    "    load: points_times = points_times()\n",
    "    dump: points_times = points_times()\n",
    "\n",
    "class stats(BaseModel): #Represents actual data\n",
    "    all_positions: list[Position] = []  # Positions recorded during a day\n",
    "    load: points_times = points_times() # Load points and times\n",
    "    dump: points_times = points_times() # Dump points and times\n",
    "    day_speeds: list[float] = []        # Speeds\n",
    "    day_acceleration: list[float] = []\n",
    "    day_dists: list[float] = []         # Distances between each recording\n",
    "    day_times: list[datetime] = []      # Timestamp for two above lists\n",
    "    inner_prods: list[float] = []       # Inner product of consecutive normalized driving vectors\n",
    "    # lat1_minus_lat0 = []\n",
    "    # lon1_minus_lon0 = []\n",
    "\n",
    "\n",
    "class automated_load_dump_for_machine():\n",
    "\n",
    "    def __init__(self, \n",
    "                 machine_data: Machine): \n",
    "                #  load_cluster_centers: typing.Any,# Both should be list[tuple[float, float]], should implement \n",
    "                #  dump_cluster_centers: typing.Any) -> None:\n",
    "        \n",
    "        self.machine = machine_data\n",
    "        self.predicted = predicted_load_dump()\n",
    "        self.stats = stats()\n",
    "\n",
    "        all_pos = [trips.positions for trips in self.machine.trips]\n",
    "        self.stats.all_positions = [item for sublist in all_pos for item in sublist]\n",
    "        self.stats.load.points = [trips.load_latlon for trips in self.machine.trips]\n",
    "        self.stats.load.times = [trips.positions[0].timestamp for trips in self.machine.trips]\n",
    "        self.stats.dump.points = [trips.dump_latlon for trips in self.machine.trips]\n",
    "        \n",
    "        actual_dump_times = []\n",
    "        for t in self.machine.trips:        #Not pretty, because we don't have dump time in trip info by default\n",
    "            temp_dump_laton = t.dump_latlon # Must match latlons\n",
    "            for position in t.positions:\n",
    "                if temp_dump_laton == (position.lat, position.lon):\n",
    "                    actual_dump_times.append(position.timestamp)\n",
    "                    break\n",
    "        self.stats.dump.times = actual_dump_times\n",
    "               \n",
    "        # self.meters_from_last_act = [] \n",
    "        # self.seconds_since_last_act = [] \n",
    "        # self.is_next_load = []          \n",
    "\n",
    "\n",
    "    def get_data(self):\n",
    "        \n",
    "        #We keep track of how many meters we have driven since last dump or load    \n",
    "        meters_since_last_activity = 0\n",
    "        time_since_last_activity = 0\n",
    "        # add initial values where we have the load\n",
    "        # self.meters_from_last_act.append(0)\n",
    "        # self.seconds_since_last_act.append(0)\n",
    "        # self.is_next_load.append(0)\n",
    "        self.stats.day_times.append(self.stats.all_positions[0].timestamp)\n",
    "        # speed is added in the loop\n",
    "\n",
    "        #We start predicting. Are going to iterate over all positions, from first to last\n",
    "        ###\n",
    "        ###\n",
    "        for i in range(1,len(self.stats.all_positions)-1):\n",
    "            \n",
    "            next_pos = self.stats.all_positions[i+1]\n",
    "            current_pos = self.stats.all_positions[i]\n",
    "            prev_pos = self.stats.all_positions[i-1]\n",
    "\n",
    "            #Meters driven since last timestamp\n",
    "            meters_driven = geopy.distance.geodesic((current_pos.lat, current_pos.lon), (prev_pos.lat, prev_pos.lon)).m\n",
    "            \n",
    "            meters_since_last_activity += meters_driven\n",
    "\n",
    "\n",
    "            #Meters driven since last timestamp\n",
    "\n",
    "            # this is the speed at point i-1 (forward derivative)\n",
    "            #Seconds passed since last timestamp\n",
    "            seconds_gone_i_minus_1 = (current_pos.timestamp.to_pydatetime()-prev_pos.timestamp.to_pydatetime()).total_seconds()\n",
    "            time_since_last_activity += seconds_gone_i_minus_1\n",
    "\n",
    "            seconds_gone_i = (next_pos.timestamp.to_pydatetime()-current_pos.timestamp.to_pydatetime()).total_seconds()\n",
    "            meters_driven_i = geopy.distance.geodesic((next_pos.lat, next_pos.lon), (current_pos.lat, current_pos.lon)).m\n",
    "            # if time duplicates, use a speed equal NaN\n",
    "            try:\n",
    "                speed_ms_i_minus_1 = meters_driven/seconds_gone_i_minus_1  # m/s\n",
    "                speed_ms_i = meters_driven_i/seconds_gone_i # m/s\n",
    "                \n",
    "                # speed_kmh_i_minus_1 = speed_ms_i_minus_1*3.6  # km/h\n",
    "                # speed_kmh_i = (speed_ms_i)*3.6                # km/h\n",
    "                acceleration_i_minus_1 = (speed_ms_i - speed_ms_i_minus_1) / (seconds_gone_i_minus_1) # m/s^2\n",
    "            except ZeroDivisionError:\n",
    "                # speed_kmh_i_minus_1 = np.nan\n",
    "                speed_ms_i_minus_1 = np.nan\n",
    "                acceleration_i_minus_1 = np.nan\n",
    "            \n",
    "            self.stats.day_acceleration.append(acceleration_i_minus_1)  # m/s^2\n",
    "            self.stats.day_speeds.append(speed_ms_i_minus_1)           # m/s\n",
    "            \n",
    "            self.stats.day_times.append(current_pos.timestamp)\n",
    "            \n",
    "\n",
    "            # if we have either load or dump, distance and time from last activity is set to 0\n",
    "            for sublist in [self.stats.load.points, self.stats.dump.points]:\n",
    "                if (self.stats.all_positions[i].lat, self.stats.all_positions[i].lon) in sublist:#, self.stats.dump.points]:\n",
    "                    meters_since_last_activity=0\n",
    "                    time_since_last_activity = 0\n",
    "                    \n",
    "            # self.seconds_since_last_act.append(time_since_last_activity)\n",
    "            \n",
    "            # self.meters_from_last_act.append(meters_since_last_activity/1000) # km\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "    def get_df_with_ml_data(self):\n",
    "        load_times_set = set(self.stats.load.times)\n",
    "        dump_times_set = set(self.stats.dump.times)\n",
    "        positions = self.stats.all_positions\n",
    "        latitude = [sublist.lat for sublist in positions]\n",
    "        longitude = [sublist.lon for sublist in positions]\n",
    "        uncertainty = [sublist.uncertainty for sublist in positions]\n",
    "        lat1_minus_lat0 = [latitude[i] - latitude[i-1] for i in range(1,len(latitude))]\n",
    "        lon1_minus_lon0 = [longitude[i] - longitude[i-1] for i in range(1,len(longitude))]\n",
    "        # append some value to be removed after df is constructed\n",
    "        lat1_minus_lat0.append(lat1_minus_lat0[-1])\n",
    "        lon1_minus_lon0.append(lon1_minus_lon0[-1])\n",
    "        speed_north_south = np.zeros_like(np.array(latitude))\n",
    "        speed_east_west = np.zeros_like(np.array(latitude))\n",
    "\n",
    "\n",
    "        # add some speed to the day_speeds list, as we we dont have the speed of the last data point ( see for loop)\n",
    "        # we have to add a value as the speed in the last point is not defined as it uses forward derivative\n",
    "        for _ in range(2):\n",
    "            self.stats.day_speeds.append(np.nan)\n",
    "            self.stats.day_acceleration.append(np.nan)\n",
    "        for idx in range(1,len(latitude)-1):\n",
    "            try:\n",
    "                total_seconds = (self.stats.day_times[idx].to_pydatetime() - self.stats.day_times[idx-1].to_pydatetime()).total_seconds()\n",
    "                speed_east_west[idx-1] = ((longitude[idx] - longitude[idx-1]   ) / total_seconds)\n",
    "                speed_north_south[idx-1] = ((latitude[idx] - latitude[idx-1]   ) / total_seconds)\n",
    "            except ZeroDivisionError:\n",
    "                speed_east_west[idx-1] = np.nan\n",
    "                speed_north_south[idx-1] = np.nan\n",
    "\n",
    "        # add another day time as the for loop excludes the last value\n",
    "        # this value will be removed anyways\n",
    "        self.stats.day_times.append(self.stats.day_times[-1])\n",
    "        load = [time in load_times_set for time in self.stats.day_times]\n",
    "        dump = [time in dump_times_set for time in self.stats.day_times]\n",
    "        # return True if either dump or load is True\n",
    "        output_labels = [d or l for d, l in zip(dump, load)]\n",
    "        \n",
    "        for i in range(len(output_labels)):\n",
    "            current_time = self.stats.day_times[i]  # The current timestamp\n",
    "            if current_time in self.stats.load.times:\n",
    "                output_labels[i] = \"Load\"\n",
    "            elif current_time in self.stats.dump.times:\n",
    "                output_labels[i] = \"Dump\"\n",
    "            else:\n",
    "                output_labels[i] = \"Driving\"\n",
    "\n",
    "        # #  is_next_load takes 1 if True and 0 if False\n",
    "        # # we have already added 0 to the list as the first value is Load\n",
    "        # previous_was_load = True\n",
    "        # for label in output_labels[1:]:\n",
    "        #     if label == \"Load\":\n",
    "        #         previous_was_load = True\n",
    "        #     if label == \"Dump\":\n",
    "        #         previous_was_load = False\n",
    "        #     if previous_was_load:\n",
    "        #         self.is_next_load.append(0)\n",
    "        #     if not previous_was_load:\n",
    "        #         self.is_next_load.append(1)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print('len(speed) :', len(self.stats.day_speeds))\n",
    "        # print('len(acceleration)', len(self.stats.day_acceleration))\n",
    "        # print('len lon: ', len(longitude))\n",
    "        # print('len lat:' , len(latitude))\n",
    "        # print('Load', len(load))\n",
    "        # print('dump', len(dump))\n",
    "        # print('datetime', len(self.stats.day_times))\n",
    "        # print('machine id', self.machine.machine_id)\n",
    "        # # print('meters from last act', len(self.meters_from_last_act))\n",
    "        # print(\"output_labels\", len(output_labels))\n",
    "        # print(\"is_next_load\", self.is_next_load)\n",
    "        # print(\"seconds from last act\", len(self.seconds_since_last_act))\n",
    "        #  create Dataframe with the given variables\n",
    "        # we dont have the speed for the last datapoint as it uses forward derivative scheme\n",
    "        df = pd.DataFrame({\n",
    "            \"MachineID\": [self.machine.machine_id]*len(self.stats.day_times),\n",
    "            \"DateTime\": self.stats.day_times,\n",
    "            # \"Time_from_start\": [(time.min - self.stats.day_times[0].min) for time in self.stats.day_times],\n",
    "            \"Speed\": self.stats.day_speeds,\n",
    "            \"Acceleration\": self.stats.day_acceleration,\n",
    "            # \"Inner_products\": self.stats.inner_prods,\n",
    "            \"Latitude\": latitude,\n",
    "            \"Longitude\": longitude,\n",
    "            \"Uncertainty\": uncertainty,\n",
    "            \"Lat1_minus_lat0\": lat1_minus_lat0,\n",
    "            \"Lon1_minus_lon0\": lon1_minus_lon0,\n",
    "            \"speed_north_south\": speed_north_south,\n",
    "            \"speed_east_west\": speed_east_west,\n",
    "            # \"km_from_last_event\": self.meters_from_last_act,\n",
    "            # \"seconds_from_last_event\": self.seconds_since_last_act,\n",
    "            # \"is_next_load\": self.is_next_load,\n",
    "            \"output_labels\": output_labels\n",
    "        })\n",
    "        \n",
    "        # filter df to remove the rows after the last dump\n",
    "        last_row = df.query('output_labels == \"Dump\"').index[-1]\n",
    "        \n",
    "        df = df.loc[:last_row]\n",
    "        \n",
    "        return df\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "days = [csv_file.split(\".csv\")[0] for csv_file in os.listdir(\"data/GPSData/trips\")]\n",
    "machine_type = \"Truck\"\n",
    "\n",
    "def save_dfs_with_ml_data() -> None:\n",
    "    \"\"\"\n",
    "    Get all the wanted data (speed, vector) for all vehicles at all days\n",
    "    \"\"\"\n",
    "    n_days = 1\n",
    "    df_training_all = pd.DataFrame()\n",
    "    df_testing_all = pd.DataFrame()\n",
    "    for day in days[:n_days]:\n",
    "        trip = dataloader.TripsLoader(day)\n",
    "        for unique_vehicle in trip._machines.keys():\n",
    "            temp_machine = trip._machines[unique_vehicle]\n",
    "            if temp_machine.machine_type == machine_type:\n",
    "                # machine_of_interest = trip._machines[unique_vehicle]\n",
    "                automated_for_given_machine = automated_load_dump_for_machine(temp_machine)\n",
    "                automated_for_given_machine.get_data()\n",
    "                df_vehicle = automated_for_given_machine.get_df_with_ml_data()\n",
    "                \n",
    "                X, y = df_vehicle.drop([\"output_labels\"],axis=1), df_vehicle[\"output_labels\"]\n",
    "                # each vehicle should be represented 20% for each day in the test data\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\n",
    "\n",
    "                df_training = pd.concat([X_train,y_train], axis=1).sort_values(by=\"DateTime\")\n",
    "                # grouped = df_training.groupby(df_training.index //3)\n",
    "                # df_training.drop(\"DateTime\")\n",
    "                # df_training.insert(1,\"DateTime start\": d)\n",
    "\n",
    "                df_training_all = pd.concat([df_training_all, df_training], axis=0)\n",
    "\n",
    "                # add the training and testing data to the total dataframe by row\n",
    "                df_testing = pd.concat([X_test, y_test], axis=1).sort_values(by=\"DateTime\")\n",
    "                df_testing_all = pd.concat([df_testing_all, df_testing], axis=0)\n",
    "\n",
    "    \n",
    "    df_training_all.dropna(inplace=True)\n",
    "    df_testing_all.dropna(inplace=True)\n",
    "    \n",
    "    # df_training_all.to_csv(f\"data/ml_model_data/training_data/train_{n_days}_days_all_trucks_multi_new_feat.csv\", sep=',', index=False)\n",
    "    # df_testing_all.to_csv(f\"data/ml_model_data/testing_data/test_{n_days}_days_all_trucks_multi_new_feat.csv\", sep=',', index=False)\n",
    "    \n",
    "    # df_training_all.to_csv(f\"data/ml_model_data/training_data/train_{n_days}_days_all_trucks_merged_datetimes.csv\", sep=',', index=False)\n",
    "    # df_testing_all.to_csv(f\"data/ml_model_data/testing_data/test_{n_days}_days_all_trucks_merged_datetimes.csv\", sep=',', index=False)\n",
    "\n",
    "\n",
    "\n",
    "save_dfs_with_ml_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_datapoints()->None:\n",
    "#     df_training = pd.read_csv(f\"data/ml_model_data/training_data/train_10_days_all_trucks_multi_new_feat.csv\", sep=',')\n",
    "#     df_testing = pd.read_csv(f\"data/ml_model_data/testing_data/test_10_days_all_trucks_multi_new_feat.csv\", sep=',')\n",
    "#     # merge dfs again and include only the first two columns\n",
    "#     df_all = pd.concat([df_training, df_testing], axis=0).iloc[:, :2].sort_values(by=[\"DateTime\",\"MachineID\"])\n",
    "  \n",
    "#     grouped = df_all.groupby(df_all.index // 3)\n",
    "#     print(grouped[\"DateTime\"].min())\n",
    "#     print(grouped[\"DateTime\"].max())\n",
    "\n",
    "#     new_df = pd.DataFrame({\"MachineID\": grouped[\"MachineID\"].first(), \"DateTime start\": grouped['DateTime'].min(), \"DateTime finish\": grouped['DateTime'].max()})\n",
    "#     new_df.to_csv(\"data/ml_model_data/testingdata.csv\", index=False, sep=\",\")\n",
    "    \n",
    "# merge_datapoints()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class points_times(BaseModel):\n",
    "#     points: list[tuple[float, float]] = []\n",
    "#     times: list[datetime] = []\n",
    "\n",
    "# class predicted_load_dump(BaseModel):\n",
    "#     load: points_times = points_times()\n",
    "#     dump: points_times = points_times()\n",
    "\n",
    "# class stats(BaseModel): #Represents actual data\n",
    "#     all_positions: list[Position] = []  # Positions recorded during a day\n",
    "#     load: points_times = points_times() # Load points and times\n",
    "#     dump: points_times = points_times() # Dump points and times\n",
    "#     day_speeds: list[float] = []        # Speeds\n",
    "#     day_dists: list[float] = []         # Distances between each recording\n",
    "#     day_times: list[datetime] = []      # Timestamp for two above lists\n",
    "#     inner_prods: list[float] = []       # Inner product of consecutive normalized driving vectors\n",
    "\n",
    "\n",
    "# class automated_load_dump_for_machine():\n",
    "\n",
    "#     def __init__(self, \n",
    "#                  machine_data: Machine): \n",
    "#                 #  load_cluster_centers: typing.Any,# Both should be list[tuple[float, float]], should implement \n",
    "#                 #  dump_cluster_centers: typing.Any) -> None:\n",
    "        \n",
    "#         self.machine = machine_data\n",
    "#         self.predicted = predicted_load_dump()\n",
    "#         self.stats = stats()\n",
    "\n",
    "#         all_pos = [trips.positions for trips in self.machine.trips]\n",
    "#         self.stats.all_positions = [item for sublist in all_pos for item in sublist]\n",
    "#         self.stats.load.points = [trips.load_latlon for trips in self.machine.trips]\n",
    "#         self.stats.load.times = [trips.positions[0].timestamp for trips in self.machine.trips]\n",
    "#         self.stats.dump.points = [trips.dump_latlon for trips in self.machine.trips]\n",
    "        \n",
    "#         actual_dump_times = []\n",
    "#         for t in self.machine.trips:        #Not pretty, because we don't have dump time in trip info by default\n",
    "#             temp_dump_laton = t.dump_latlon # Must match latlons\n",
    "#             for position in t.positions:\n",
    "#                 if temp_dump_laton == (position.lat, position.lon):\n",
    "#                     actual_dump_times.append(position.timestamp)\n",
    "#                     break\n",
    "#         self.stats.dump.times = actual_dump_times\n",
    "\n",
    "#         # self.load_cluster_centers = load_cluster_centers\n",
    "#         # self.dump_cluster_centers = dump_cluster_centers\n",
    "\n",
    "#         #These four lines should be rewritten, to a class or something\n",
    "#         self.entering_load_working_area = []\n",
    "#         self.exiting_load_working_area = []\n",
    "#         self.entering_dump_working_area = []\n",
    "#         self.exiting_dump_working_area = []\n",
    "#         self.in_dumping_area: list[int | bool] = []\n",
    "#         self.in_loading_area: list[int | bool] = []\n",
    "#         self.meters_from_last_act = []  \n",
    "#         self.is_next_load = []          \n",
    "\n",
    "#     def predict(self):\n",
    "\n",
    "#         #We know first loading because that is when data begins\n",
    "#         self.predicted.load.points.append((self.stats.all_positions[0].lat, self.stats.all_positions[0].lon))\n",
    "#         self.predicted.load.times.append(self.stats.all_positions[0].timestamp)\n",
    "\n",
    "#         #When true, we are predicting load, else dump. Next prediction will be dump, since we have load above\n",
    "#         predicting_load = False\n",
    "\n",
    "#         #Initialize variables that keep track of whether or not we are in a usual area for loading or dumping\n",
    "#         #As given by load and dump clusters and criterias.meters_from_area\n",
    "#         in_load_working_area = False #Probably true, since first position is when loading\n",
    "#         in_dump_working_area = False #Maybe false, since first position is when loading\n",
    "\n",
    "        \n",
    "#         #We determine the true value of the two above variables\n",
    "#         # for coord in self.load_cluster_centers:  #But verify with created clusters of load points from day before\n",
    "#         #         if geopy.distance.geodesic(coord, (self.stats.all_positions[0].lat, self.stats.all_positions[0].lon)).m < criterias.meters_from_area:\n",
    "#         #             in_load_working_area = True\n",
    "#         #             self.in_loading_area.append(1)\n",
    "#         #         else:\n",
    "#         #             self.in_loading_area.append(0)\n",
    "\n",
    "#         # for coord in self.dump_cluster_centers: #But verify with created clusters of load points from day before\n",
    "#         #         if geopy.distance.geodesic(coord, (self.stats.all_positions[0].lat, self.stats.all_positions[0].lon)).m < criterias.meters_from_area:\n",
    "#         #             in_dump_working_area = True\n",
    "#         #             self.in_dumping_area.append(1)\n",
    "#         #         else:\n",
    "#         #             self.in_dumping_area.append(0)\n",
    "\n",
    "        \n",
    "\n",
    "#         #We keep track of how many meters we have driven since last dump or load    \n",
    "#         meters_since_last_activity = 0\n",
    "        \n",
    "#         # add initial values where we have the load\n",
    "#         self.meters_from_last_act.append(0)\n",
    "#         self.is_next_load.append(0)\n",
    "#         self.stats.day_times.append(self.stats.all_positions[0].timestamp)\n",
    "#         # speed is added in the loop\n",
    "\n",
    "#         #We start predicting. Are going to iterate over all positions, from first to last\n",
    "#         for i in range(1,len(self.stats.all_positions)):\n",
    "            \n",
    "#             current_pos = self.stats.all_positions[i]\n",
    "#             prev_pos = self.stats.all_positions[i-1]\n",
    "\n",
    "#             #Seconds passed since last timestamp\n",
    "#             seconds_gone = (current_pos.timestamp.to_pydatetime()-prev_pos.timestamp.to_pydatetime()).total_seconds()\n",
    "#             if seconds_gone <= 0:\n",
    "#                 # self.stats.day_speeds.append(self.stats.day_speeds[-1])\n",
    "#                 # self.stats.inner_prods.append(self.stats.inner_prods[-1])\n",
    "#                 # # self.stats.inner_prods.append(self.stats.inner_prods[-1])\n",
    "#                 # self.stats.day_times.append(self.stats.day_times[-1])\n",
    "\n",
    "#                 # just add some unique value so that we can remove these duplicate rows later\n",
    "#                 self.stats.day_speeds.append(np.nan)\n",
    "#                 # self.stats.inner_prods.append(np.nan)\n",
    "#                 # self.stats.inner_prods.append(self.stats.inner_prods[-1])\n",
    "#                 self.stats.day_times.append(self.stats.day_times[-1])\n",
    "#                 self.meters_from_last_act.append(np.nan)\n",
    "#             if seconds_gone > 0:\n",
    "\n",
    "#                 #Meters driven since last timestamp\n",
    "#                 meters_driven = geopy.distance.geodesic((current_pos.lat, current_pos.lon), (prev_pos.lat, prev_pos.lon)).m\n",
    "                \n",
    "#                 meters_since_last_activity += meters_driven\n",
    "#                 # if we have either load or dump, distance from last activity is set to 0\n",
    "#                 for sublist in [self.stats.load.points, self.stats.dump.points]:\n",
    "#                     if (self.stats.all_positions[i].lat, self.stats.all_positions[i].lon) in sublist:#, self.stats.dump.points]:\n",
    "#                         meters_since_last_activity=0\n",
    "                    \n",
    "                    \n",
    "#                 self.meters_from_last_act.append(meters_since_last_activity/1000) # km\n",
    "\n",
    "#                 #Meters driven since last timestamp\n",
    "\n",
    "#                 # this is the speed at point i-1 (forward derivative)\n",
    "#                 speed_kmh = (meters_driven/seconds_gone)*3.6\n",
    "\n",
    "#                 #Add the speed to a list for entire day\n",
    "#                 self.stats.day_speeds.append(speed_kmh)\n",
    "\n",
    "#                 #Add the distance (km) between the two timestamps \n",
    "#                 # self.stats.day_dists.append(meters_driven/1000)\n",
    "\n",
    "#                 #Add the timestamp for the two above values\n",
    "#                 self.stats.day_times.append(current_pos.timestamp)\n",
    "\n",
    "#                 #Compute vectors. This is a lot of code, maybe create some function?\n",
    "#                 #Create vector for computing reverse of vehicle\n",
    "#                 # vector_start = current_pos.timestamp-timedelta(seconds=criterias.seconds_for_vector)\n",
    "#                 # index_start_vector = 0\n",
    "#                 # for j, ts in enumerate(self.stats.day_times):\n",
    "#                 #     if ts >= vector_start:\n",
    "#                 #         index_start_vector = j\n",
    "#                 #         break\n",
    "                \n",
    "#                 # current_vector = [self.stats.all_positions[i].lat-self.stats.all_positions[i-3].lat,\n",
    "#                 #                     self.stats.all_positions[i].lon-self.stats.all_positions[i-3].lon]\n",
    "#                 # prev_vector = [self.stats.all_positions[index_start_vector].lat-self.stats.all_positions[index_start_vector-3].lat,\n",
    "#                 #                 self.stats.all_positions[index_start_vector].lon-self.stats.all_positions[index_start_vector-3].lon]\n",
    "\n",
    "#                 # current_vector_norm = current_vector/np.linalg.norm(current_vector)\n",
    "#                 # prev_vector_norm = prev_vector/np.linalg.norm(prev_vector)\n",
    "#                 # inner_product = np.inner(current_vector_norm,prev_vector_norm)\n",
    "#                 # self.stats.inner_prods.append(inner_product)\n",
    "            \n",
    "#                 #Check if we are currently in a loading or dumping working area\n",
    "#                 #If yes, we update value\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "#                 # currently_in_load_working_area = False\n",
    "#                 # for coord in self.load_cluster_centers:\n",
    "#                 #     if geopy.distance.geodesic(coord, (current_pos.lat, current_pos.lon)).m < criterias.meters_from_area:\n",
    "#                 #         currently_in_load_working_area = True\n",
    "                \n",
    "#                 # currently_in_dump_working_area = False\n",
    "#                 # for coord in self.dump_cluster_centers:\n",
    "#                 #     if geopy.distance.geodesic(coord, (current_pos.lat, current_pos.lon)).m < criterias.meters_from_area:\n",
    "#                 #         currently_in_dump_working_area = True\n",
    "\n",
    "#                 # #Could be interesting to mark where we enter and exit load and dump zones. Can be done with this code.\n",
    "#                 # #Can be used for plotting, see notebooks for examples    \n",
    "#                 # if not in_load_working_area and currently_in_load_working_area:\n",
    "#                 #     self.entering_load_working_area.append(current_pos.timestamp)\n",
    "#                 #     in_load_working_area = True\n",
    "#                 # elif in_load_working_area and not currently_in_load_working_area:\n",
    "#                 #     self.exiting_load_working_area.append(current_pos.timestamp)\n",
    "#                 #     in_load_working_area = False\n",
    "\n",
    "#                 # if not in_dump_working_area and currently_in_dump_working_area:\n",
    "#                 #     self.entering_dump_working_area.append(current_pos.timestamp)\n",
    "#                 #     in_dump_working_area = True\n",
    "#                 # elif in_dump_working_area and not currently_in_dump_working_area:\n",
    "#                 #     self.exiting_dump_working_area.append(current_pos.timestamp)\n",
    "#                 #     in_dump_working_area = False\n",
    "\n",
    "#                 # #Logic for predicting loading point\n",
    "#                 # if speed_kmh < criterias.speed_limit and meters_since_last_activity>criterias.meters_since_last_activity:\n",
    "#                 #     if predicting_load:\n",
    "#                 #         if in_load_working_area:\n",
    "#                 #             last_min_start = current_pos.timestamp-timedelta(minutes=criterias.minutes_load)\n",
    "#                 #             index_start_minute = 0\n",
    "#                     #         for i, ts in enumerate(self.stats.day_times):\n",
    "#                     #             if ts >= last_min_start:\n",
    "#                     #                 index_start_minute = i\n",
    "#                     #                 break\n",
    "#                     #         sum_over_last_minute = np.sum(self.stats.day_speeds[index_start_minute:])\n",
    "                            \n",
    "#                     #         if sum_over_last_minute < criterias.max_sum_last_x_minutes_load:\n",
    "#                     #             self.predicted.load.points.append((current_pos.lat, current_pos.lon))\n",
    "#                     #             self.predicted.load.times.append(current_pos.timestamp)\n",
    "                                \n",
    "#                     #             #Have now predicted a load, next dump\n",
    "#                     #             predicting_load = False\n",
    "#                     #             meters_since_last_activity = 0\n",
    "\n",
    "#                     # else: #Predicting dump\n",
    "#                     #     if in_dump_working_area:\n",
    "#                     #         last_min_start = current_pos.timestamp-timedelta(minutes=criterias.minutes_dump)\n",
    "#                     #         index_start_minute = 0\n",
    "#                     #         for i, ts in enumerate(self.stats.day_times):\n",
    "#                     #             if ts >= last_min_start:\n",
    "#                     #                 index_start_minute = i\n",
    "#                     #                 break\n",
    "#                     #         sum_over_last_minute = np.sum(self.stats.day_speeds[index_start_minute:])\n",
    "                            \n",
    "#                     #         if sum_over_last_minute < criterias.max_sum_last_x_minutes_dump and self.stats.inner_prods[-1] < criterias.inner_prod_threshold: #Not ideal, want to instead pick the best among times, not just the first viable option, but difficult with live tracking.\n",
    "#                     #             self.predicted.dump.points.append((current_pos.lat, current_pos.lon))\n",
    "#                     #             self.predicted.dump.times.append(current_pos.timestamp)\n",
    "                                \n",
    "#                     #             #Have now predicted a dump, next load\n",
    "#                     #             predicting_load = True\n",
    "#                     #             meters_since_last_activity = 0\n",
    "\n",
    "#     def get_df_with_ml_data(self):\n",
    "#         load_times_set = set(self.stats.load.times)\n",
    "#         dump_times_set = set(self.stats.dump.times)\n",
    "#         positions = self.stats.all_positions\n",
    "#         latitude = [sublist.lat for sublist in positions]\n",
    "#         longitude = [sublist.lon for sublist in positions]\n",
    "#         load = [time in load_times_set for time in self.stats.day_times]\n",
    "#         dump = [time in dump_times_set for time in self.stats.day_times]\n",
    "#         print('len(speed) :', len(self.stats.day_speeds))\n",
    "#         print('len lon: ', len(longitude))\n",
    "#         print('len lat:' , len(latitude))\n",
    "#         print('Load', len(load))\n",
    "#         print('dump', len(dump))\n",
    "#         print('datetime', len(self.stats.day_times))\n",
    "#         print('machine id', self.machine.machine_id)\n",
    "#         print('meters from last act', len(self.meters_from_last_act[:-1]))\n",
    "#         #  create Dataframe with the given variables\n",
    "#         # we dont have the speed for the last datapoint as it uses forward derivative scheme\n",
    "#         df = pd.DataFrame({\n",
    "#             \"MachineID\": [self.machine.machine_id]*len(self.stats.day_times),\n",
    "#             \"DateTime\": self.stats.day_times,\n",
    "#             # \"Time_from_start\": [(time.min - self.stats.day_times[0].min) for time in self.stats.day_times],\n",
    "#             \"Speed\": self.stats.day_speeds,\n",
    "#             # \"Inner_products\": self.stats.inner_prods,\n",
    "#             \"Latitude\": latitude,\n",
    "#             \"Longitude\": longitude,\n",
    "#             \"km_from_last_event\": self.meters_from_last_act,\n",
    "#             \"Load\": load,\n",
    "#             \"Dump\": dump\n",
    "#         })\n",
    "\n",
    "#         # filter df to remove the rows after the last dump (True,False)\n",
    "#         last_row = df.query('A == True and B == False').index[-1]\n",
    "#         df = df.loc[:last_row]\n",
    "        \n",
    "#         return df\n",
    "#         # df.to_csv(f'data/ml_model_data/time_speed_{self.machine.machine_id}.csv', index=False, sep=',')\n",
    "\n",
    "#         # # Save units to csv file\n",
    "#         # units = pd.DataFrame({\n",
    "#         #     \"DateTime\": [\"Datetime\"],\n",
    "#         #     \"Time_from_start\": [\"Minutes\"],\n",
    "#         #     \"Speed\": [\"km/h\"],\n",
    "#         #     \"Inner_products\": [\"-\"],\n",
    "#         #     \"Load\": [\"-\"],\n",
    "#         #     \"Dump\": [\"-\"]\n",
    "#         # })\n",
    "#         # units.to_csv(f'data/ml_model_data/units_{self.machine.machine_id}.csv', index=False, sep=',')\n",
    "\n",
    "    \n",
    "#     def time_plot(self):\n",
    "\n",
    "#         # Plots, not wanted when a lot of data\n",
    "#         # Create subplots with 2 rows and 1 column\n",
    "#         fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.1)\n",
    "\n",
    "#         # Add the first line plot to the first subplot\n",
    "        \n",
    "#         fig.add_trace(go.Scatter(x=self.stats.day_times, y=self.stats.day_speeds, mode='lines', name='Speed'), row=1, col=1)\n",
    "#         fig.add_trace(go.Scatter(x=self.stats.load.times, y=[0 for a in self.stats.load.times], mode='markers', marker=dict(symbol='cross', size=10, color='red'), name='Load actual'), row=1, col=1)\n",
    "#         fig.add_trace(go.Scatter(x=self.predicted.load.times, y=[0 for p in self.predicted.load.times], mode='markers', marker=dict(symbol='star', size=10, color='red'), name='Load predicted'), row=1, col=1)\n",
    "#         fig.add_trace(go.Scatter(x=self.stats.dump.times, y=[0 for a in self.stats.dump.times], mode='markers', marker=dict(symbol='cross', size=10, color='green'), name='Dump actual'), row=1, col=1)\n",
    "#         fig.add_trace(go.Scatter(x=self.predicted.dump.times, y=[0 for p in self.predicted.dump.times], mode='markers', marker=dict(symbol='star', size=10, color='green'), name='Dump predicted'), row=1, col=1)\n",
    "\n",
    "#         # Add the second line plot to the second subplot\n",
    "#         fig.add_trace(go.Scatter(x=self.stats.day_times, y=np.cumsum(self.stats.day_dists), mode='lines', name='Cumulative distance'), row=2, col=1)\n",
    "#         fig.add_trace(go.Scatter(x=self.stats.load.times, y=[0 for a in self.stats.load.times], mode='markers', marker=dict(symbol='cross', size=10, color='red'), name='Load actual'), row=2, col=1)\n",
    "#         fig.add_trace(go.Scatter(x=self.predicted.load.times, y=[0 for p in self.predicted.load.times], mode='markers', marker=dict(symbol='star', size=10, color='red'), name='Load predicted'), row=2, col=1)\n",
    "#         fig.add_trace(go.Scatter(x=self.stats.dump.times, y=[0 for a in self.stats.dump.times], mode='markers', marker=dict(symbol='cross', size=10, color='green'), name='Dump actual'), row=2, col=1)\n",
    "#         fig.add_trace(go.Scatter(x=self.predicted.dump.times, y=[0 for p in self.predicted.dump.times], mode='markers', marker=dict(symbol='star', size=10, color='green'), name='Dump predicted'), row=2, col=1)\n",
    "\n",
    "#         # Add the third line plot\n",
    "#         fig.add_trace(go.Scatter(x=self.stats.day_times, y=self.stats.inner_prods, mode='lines', name='Inner product of vectors'), row=3, col=1)\n",
    "\n",
    "#         # Update layout settings for both subplots\n",
    "#         fig.update_layout(title=str('Subplots of Speeds and cumulative distance, machine_id: '+ str(self.machine.machine_id)),\n",
    "#                         xaxis_title='Timestamp',\n",
    "#                         showlegend=True)\n",
    "\n",
    "#         fig.show()\n",
    "\n",
    "#     def gantt_plot(self):\n",
    "\n",
    "#         #Actual trips\n",
    "#         all_trips_for_machine = self.machine.trips\n",
    "#         start_end_each_trip_dict_actual = [dict(Start=trips.start_date, End=trips.end_date, Load=trips.load, Dist=trips.length, Id=trips.trip_id) for trips in all_trips_for_machine]\n",
    "#         df_pyplot_actual = pd.DataFrame(start_end_each_trip_dict_actual)\n",
    "        \n",
    "        \n",
    "#         #Predicted trips\n",
    "#         start_end_each_trip_dict_predicted = [dict(Start=self.predicted.load.times[i], End=self.predicted.load.times[i+1]) for i in range(len(self.predicted.load.times)-1)]\n",
    "#         df_pyplot_predicted = pd.DataFrame(start_end_each_trip_dict_predicted)\n",
    "\n",
    "#         fig = px.timeline(df_pyplot_actual, x_start=\"Start\", x_end=\"End\", custom_data=[\"Load\", \"Dist\", \"Id\"])\n",
    "#         fig.update_traces(\n",
    "#             hovertemplate=\"<br>\".join([\n",
    "#                 \"Start: %{base}\",\n",
    "#                 \"End: %{x}\",\n",
    "#                 \"Load: %{customdata[0]}\",\n",
    "#                 \"Distance: %{customdata[1]}\",\n",
    "#                 \"ID: %{customdata[2]}\"\n",
    "#             ])\n",
    "#         )\n",
    "#         fig.update_yaxes(autorange=\"reversed\") # otherwise tasks are listed from the bottom up\n",
    "#         fig.show()\n",
    "\n",
    "#         fig = px.timeline(df_pyplot_predicted, x_start=\"Start\", x_end=\"End\")\n",
    "#         fig.update_traces(\n",
    "#             hovertemplate=\"<br>\".join([\n",
    "#                 \"Start: %{base}\",\n",
    "#                 \"End: %{x}\"\n",
    "#             ])\n",
    "#         )\n",
    "#         fig.update_yaxes(autorange=\"reversed\") # otherwise tasks are listed from the bottom up\n",
    "#         fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
